#!/usr/bin/env python3
"""
generate_textbook_json.py
Reads a sequence of markdown files (chapter/appendix/frontmatter) from the current folder
and produces COMPLETE_TEXTBOOK_JSON_ALIGNED.json and a .md wrapper.

Usage:
    python3 generate_textbook_json.py

Make sure you run it from:
 /Users/akashchatake/Downloads/Chatake-Innoworks-Organization/Projects/Publications/Machine-Learning-Textbook/
"""
import os
import json
from pathlib import Path

# ---- CONFIG: order of files to include (edit if needed) ----
BASE = Path(".").resolve()

ordered_files = [
    # Use your actual files that exist
    "TITLE_PAGE.md",
    "COPYRIGHT.md", 
    "DEDICATION.md",
    "ABOUT_AUTHOR.md",
    "PREFACE.md",
    "TABLE_OF_CONTENTS.md",
    # chapters (using your actual files)
    "chapters/chapter_01_introduction.md",
    "chapters/chapter_02_data_preprocessing.md",
    "chapters/chapter_03_feature_engineering.md",
    "chapters/chapter_04_classification.md",
    "chapters/chapter_05_regression.md",
    "chapters/chapter_06_clustering.md",
    "chapters/chapter_07_dimensionality_reduction.md",
    "chapters/chapter_08_end_to_end_projects.md",
    "chapters/chapter_09_model_selection_evaluation.md",
    "chapters/chapter_10_ethics_deployment.md",
    # appendices (using your actual files)
    "appendices/appendix_a_python_setup.md",
    "appendices/appendix_b_mathematical_foundations.md",
    "appendices/appendix_c_datasets_resources.md",
    "appendices/appendix_d_evaluation_metrics.md",
    "appendices/appendix_e_industry_applications.md",
    # backmatter 
    "EPILOGUE.md",
    "REFERENCES_BIBLIOGRAPHY.md",
    "INDEX.md",
    "BACK_COVER.md",
]

# ---- metadata (edit if you want different values) ----
metadata = {
    "title": "Machine Learning: Foundations & Futures",
    "subtitle": "A Comprehensive Guide to Artificial Intelligence and Data Science",
    "author": "Akash Chatake (MindforgeAI / Chatake Innoworks Pvt. Ltd.)",
    "imprint": "MindforgeAI Press / Chatake Innoworks Publications",
    "edition": "Founder's Edition, First Edition",
    "year": 2025,
    "language": "en-US",
    "course_code": "MSBTE 316316",
    "target_audience": "Diploma & Undergraduate Computer Technology & Engineering Students",
    "pages_estimate": 500,
    "keywords": ["Machine Learning", "AI", "MSBTE", "Data Science", "MindforgeAI"],
    "rights": "© 2025 Akash Chatake / Chatake Innoworks Organization. All rights reserved."
}

# helper to read file safely as UTF-8
def read_file_utf8(path: Path):
    if not path.exists():
        return None, f"Missing: {path}"
    # try reading as utf-8, fall back to latin1 then replace
    try:
        text = path.read_text(encoding="utf-8")
        return text, None
    except UnicodeDecodeError:
        try:
            text = path.read_text(encoding="latin-1")
            # convert to utf-8 string
            return text, None
        except Exception as e:
            return None, f"Error reading {path}: {e}"
    except Exception as e:
        return None, f"Error reading {path}: {e}"

def build_json():
    result = {
        "metadata": metadata,
        "front_matter": {},
        "parts": [],
        "appendices": [],
        "back_matter": {},
        "publishing": {
            "cover": {"file": "ML_BOOK_Covers (2).pdf", "position": "front"},
            "isbn_print": None,
            "isbn_digital": None,
            "publisher_notes": "MindforgeAI Press — Founder's Edition"
        },
        "notes": {
            "usage": "This JSON was auto-generated by generate_textbook_json.py. Each item includes 'source_file' and 'content'.",
            "escape_rules": "Content is stored raw. Use a JSON parser to extract markdown."
        }
    }

    missing = []
    files_included = []

    # Simple approach: treat frontmatter files first if they exist
    front_keys = [
        ("title_page", "TITLE_PAGE.md"),
        ("copyright_publication", "COPYRIGHT.md"),
        ("dedication", "DEDICATION.md"),
        ("about_the_author", "ABOUT_AUTHOR.md"),
        ("preface", "PREFACE.md"),
        ("table_of_contents", "TABLE_OF_CONTENTS.md"),
    ]
    for key, rel in front_keys:
        p = BASE / rel
        text, err = read_file_utf8(p)
        if err:
            result["front_matter"][key] = {"source_file": rel, "content": None, "note": err}
            missing.append(rel)
        else:
            result["front_matter"][key] = {"source_file": rel, "content": text}
            files_included.append(rel)

    # Now create a parts/chapters list by iterating ordered_files and grouping chapters simply
    # We'll create a single "Part: Combined" and add each file as a chapter for simplicity,
    # preserving order and filename
    part = {"part_number": 1, "part_title": "Full Book (Merged)", "chapters": []}
    chapter_counter = 0
    for rel in ordered_files:
        p = BASE / rel
        text, err = read_file_utf8(p)
        chapter_counter += 1
        chap_obj = {
            "chapter_number": chapter_counter,
            "chapter_title": Path(rel).stem,
            "source_file": rel,
            "content": None
        }
        if err:
            chap_obj["note"] = err
            missing.append(rel)
        else:
            # trim BOM if present
            if text.startswith("\ufeff"):
                text = text.lstrip("\ufeff")
            chap_obj["content"] = text
            files_included.append(rel)

        part["chapters"].append(chap_obj)

    result["parts"].append(part)

    # appendices - handled in ordered_files now, so skip this section

    # back matter
    for rel in ["EPILOGUE.md", "REFERENCES_BIBLIOGRAPHY.md", "INDEX.md", "BACK_COVER.md"]:
        p = BASE / rel
        text, err = read_file_utf8(p)
        key = Path(rel).stem
        if err:
            result["back_matter"][key] = {"source_file": rel, "content": None, "note": err}
            missing.append(rel)
        else:
            result["back_matter"][key] = {"source_file": rel, "content": text}
            files_included.append(rel)

    return result, files_included, missing

def write_outputs(json_obj):
    out_json = Path("COMPLETE_TEXTBOOK_JSON_ALIGNED.json")
    out_md = Path("COMPLETE_TEXTBOOK_JSON_ALIGNED.md")
    with out_json.open("w", encoding="utf-8") as f:
        json.dump(json_obj, f, ensure_ascii=False, indent=2)
    # also produce a markdown-wrapped JSON for convenience
    with out_md.open("w", encoding="utf-8") as f:
        f.write("```json\n")
        json.dump(json_obj, f, ensure_ascii=False, indent=2)
        f.write("\n```\n")

if __name__ == "__main__":
    print("Scanning files and building JSON ... (cwd: {})".format(BASE))
    json_obj, included, missing = build_json()
    write_outputs(json_obj)
    print("Wrote COMPLETE_TEXTBOOK_JSON_ALIGNED.json and COMPLETE_TEXTBOOK_JSON_ALIGNED.md")
    if included:
        print("Files included ({}):".format(len(included)))
        for s in included:
            print("  ✓", s)
    if missing:
        print("\nMissing or unreadable files ({}). These entries are left with 'note' in JSON:".format(len(missing)))
        for s in missing:
            print("  ✗", s)
    else:
        print("\nAll listed files found and embedded.")
    print("\nDone.")
