# Chapter 9: The Judge and Jury - Model Selection and Evaluation

## Learning Outcomes: Becoming the Supreme Court of Machine Learning
By the end of this chapter, you will have transcended from model builder to **algorithmic arbiter**:
- Orchestrate sophisticated cross-validation symphonies that reveal truth beyond randomness
- Architect evaluation frameworks that separate genuine intelligence from statistical accidents
- Wield statistical significance testing as your sword of scientific truth
- Navigate the treacherous waters of imbalanced data, temporal dependencies, and domain constraints
- Build automated model selection systems that think and adapt like experienced data scientists
- Master the art of nested cross-validation‚Äîthe Zen of unbiased performance estimation
- Craft custom metrics that speak the language of business value and human impact

## Chapter Overview: The Philosophy of Algorithmic Truth

*"All models are wrong, but some are useful. The art is knowing which ones."* ‚Äî Adapted from George Box

Welcome to the most crucial chapter in your machine learning journey‚Äîwhere we transform from optimistic model builders into **rigorous evaluators of algorithmic truth**. This is where the rubber meets the road, where dreams of perfect predictions encounter the harsh but beautiful reality of statistical validation.

### The Sacred Responsibility of Model Evaluation

Imagine you're a judge in a court where the defendants are algorithms and the evidence is data. Your verdict doesn't just affect academic scores‚Äîit influences real decisions that impact real people. Will this medical diagnostic model save lives or give false hope? Will this loan approval algorithm promote fairness or perpetuate bias? Will this recommendation system delight users or trap them in filter bubbles?

This chapter is your **judicial training academy** for the algorithmic age. We don't just compare numbers‚Äîwe develop the wisdom to distinguish between models that merely memorize and those that truly understand.

### The Art and Science of Algorithmic Justice

**What awaits you in this transformative chapter:**

üéØ **Cross-Validation Mastery**: Beyond simple train-test splits to sophisticated validation strategies that honor the complexity of real-world data

üìä **Statistical Significance**: Learning to hear the whispers of true signal above the shouts of random noise  

‚öñÔ∏è **Fair Comparison Frameworks**: Building evaluation systems that give every algorithm a fair trial

üîÆ **Future-Proof Validation**: Techniques that predict not just performance, but sustainability and reliability

üé≠ **Domain-Aware Evaluation**: Adapting your judgment to the unique requirements of different industries and applications

### The Philosophy of Model Selection

This isn't just about picking the highest accuracy score‚Äîit's about developing the **intuitive wisdom** that recognizes when a model is ready for the real world. You'll learn to see beyond the surface metrics to understand the deeper questions: Does this model capture the essence of the problem? Will it degrade gracefully when conditions change? Can we trust its confidence estimates?

---

## 9.1 Advanced Cross-Validation Strategies

### 9.1.1 Beyond Standard K-Fold: Specialized CV Techniques

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import (KFold, StratifiedKFold, TimeSeriesSplit, 
                                   GroupKFold, LeaveOneGroupOut, cross_val_score)
from sklearn.metrics import make_scorer
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class AdvancedCrossValidation:
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.cv_strategies = {}
        self.results = {}
    
    def stratified_group_kfold(self, X, y, groups, n_splits=5):
        """
        Implement stratified group K-fold that maintains both group integrity
        and class distribution balance
        """
        from collections import defaultdict, Counter
        
        # Group samples by group and class
        group_class_counts = defaultdict(lambda: defaultdict(int))
        for group, label in zip(groups, y):
            group_class_counts[group][label] += 1
        
        # Convert to list of (group, class_distribution)
        groups_info = []
        for group, class_counts in group_class_counts.items():
            total_samples = sum(class_counts.values())
            class_ratios = {cls: count/total_samples for cls, count in class_counts.items()}
            groups_info.append((group, class_ratios, total_samples))
        
        # Sort groups by size for better distribution
        groups_info.sort(key=lambda x: x[2], reverse=True)
        
        # Initialize folds
        folds = [[] for _ in range(n_splits)]
        fold_class_counts = [defaultdict(int) for _ in range(n_splits)]
        
        # Assign groups to folds
        for group, class_ratios, group_size in groups_info:
            # Find fold with most similar class distribution
            best_fold = 0
            best_score = float('inf')
            
            for fold_idx in range(n_splits):
                # Calculate distribution similarity
                fold_total = sum(fold_class_counts[fold_idx].values())
                if fold_total == 0:
                    score = 0  # Empty fold, good choice
                else:
                    score = 0
                    for cls in set(class_ratios.keys()) | set(fold_class_counts[fold_idx].keys()):
                        current_ratio = fold_class_counts[fold_idx][cls] / fold_total
                        target_ratio = class_ratios.get(cls, 0)
                        score += abs(current_ratio - target_ratio)
                
                if score < best_score:
                    best_score = score
                    best_fold = fold_idx
            
            # Assign group to best fold
            folds[best_fold].append(group)
            for cls, count in group_class_counts[group].items():
                fold_class_counts[best_fold][cls] += count
        
        # Generate train/test indices
        for test_fold in range(n_splits):
            test_groups = set(folds[test_fold])
            train_indices = []
            test_indices = []
            
            for idx, group in enumerate(groups):
                if group in test_groups:
                    test_indices.append(idx)
                else:
                    train_indices.append(idx)
            
            yield train_indices, test_indices
    
    def temporal_cross_validation(self, X, y, time_column, n_splits=5, gap_size=0):
        """
        Implement time-aware cross-validation with optional gap between train/test
        """
        # Sort data by time
        time_sorted_idx = np.argsort(X[time_column])
        n_samples = len(X)
        
        # Calculate fold sizes
        test_size = n_samples // n_splits
        
        for i in range(n_splits):
            # Calculate test period
            test_start = i * test_size
            test_end = min((i + 1) * test_size, n_samples)
            
            # Apply gap
            train_end = max(0, test_start - gap_size)
            
            # Get indices
            train_indices = time_sorted_idx[:train_end].tolist()
            test_indices = time_sorted_idx[test_start:test_end].tolist()
            
            if len(train_indices) > 0 and len(test_indices) > 0:
                yield train_indices, test_indices
    
    def nested_cross_validation(self, model, param_grid, X, y, outer_cv=5, inner_cv=3, 
                              scoring='accuracy'):
        """
        Implement nested cross-validation for unbiased performance estimation
        """
        from sklearn.model_selection import GridSearchCV, cross_val_score
        
        # Outer CV for performance estimation
        outer_scores = []
        best_params_per_fold = []
        
        outer_cv_splitter = KFold(n_splits=outer_cv, shuffle=True, random_state=self.random_state)
        
        for fold_idx, (train_idx, test_idx) in enumerate(outer_cv_splitter.split(X)):
            print(f"Processing outer fold {fold_idx + 1}/{outer_cv}")
            
            X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]
            y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]
            
            # Inner CV for hyperparameter selection
            inner_cv_splitter = KFold(n_splits=inner_cv, shuffle=True, random_state=self.random_state)
            
            grid_search = GridSearchCV(
                model, param_grid, cv=inner_cv_splitter, 
                scoring=scoring, n_jobs=-1
            )
            
            # Fit on outer training set
            grid_search.fit(X_train_outer, y_train_outer)
            
            # Evaluate best model on outer test set
            best_model = grid_search.best_estimator_
            score = best_model.score(X_test_outer, y_test_outer)
            
            outer_scores.append(score)
            best_params_per_fold.append(grid_search.best_params_)
            
            print(f"  Fold {fold_idx + 1} score: {score:.4f}")
            print(f"  Best params: {grid_search.best_params_}")
        
        results = {
            'outer_scores': outer_scores,
            'mean_score': np.mean(outer_scores),
            'std_score': np.std(outer_scores),
            'best_params_per_fold': best_params_per_fold,
            'cv_scores_detailed': outer_scores
        }
        
        print(f"\nNested CV Results:")
        print(f"Mean score: {results['mean_score']:.4f} (+/- {results['std_score'] * 2:.4f})")
        
        return results
    
    def custom_cv_for_time_series(self, X, y, time_column, forecast_horizon=1, 
                                 min_train_size=None, step_size=1):
        """
        Time series cross-validation with walk-forward validation
        """
        # Sort by time
        time_sorted = X.sort_values(time_column)
        sorted_indices = time_sorted.index.tolist()
        
        n_samples = len(X)
        if min_train_size is None:
            min_train_size = n_samples // 3
        
        folds = []
        
        for start_idx in range(min_train_size, n_samples - forecast_horizon, step_size):
            train_indices = sorted_indices[:start_idx]
            test_indices = sorted_indices[start_idx:start_idx + forecast_horizon]
            
            if len(test_indices) == forecast_horizon:
                folds.append((train_indices, test_indices))
        
        print(f"Generated {len(folds)} time series CV folds")
        return folds
    
    def evaluate_cv_stability(self, model, X, y, cv_strategies, scoring='accuracy', n_repeats=5):
        """
        Evaluate stability of cross-validation results across different strategies
        """
        results = {}
        
        for strategy_name, cv_splitter in cv_strategies.items():
            strategy_scores = []
            
            for repeat in range(n_repeats):
                # Add randomness for repeated evaluation
                if hasattr(cv_splitter, 'random_state'):
                    cv_splitter.random_state = self.random_state + repeat
                
                scores = cross_val_score(model, X, y, cv=cv_splitter, scoring=scoring)
                strategy_scores.extend(scores)
            
            results[strategy_name] = {
                'scores': strategy_scores,
                'mean': np.mean(strategy_scores),
                'std': np.std(strategy_scores),
                'min': np.min(strategy_scores),
                'max': np.max(strategy_scores),
                'cv': np.std(strategy_scores) / np.mean(strategy_scores)  # Coefficient of variation
            }
        
        return results

class ModelComparisonFramework:
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.comparison_results = {}
        
    def statistical_comparison(self, model_results, alpha=0.05):
        """
        Perform statistical significance testing between models
        """
        from scipy.stats import ttest_rel, wilcoxon, friedmanchisquare
        import itertools
        
        model_names = list(model_results.keys())
        comparison_matrix = pd.DataFrame(index=model_names, columns=model_names)
        
        # Pairwise comparisons
        for model1, model2 in itertools.combinations(model_names, 2):
            scores1 = model_results[model1]['scores']
            scores2 = model_results[model2]['scores']
            
            # Paired t-test (assumes normality)
            t_stat, t_pval = ttest_rel(scores1, scores2)
            
            # Wilcoxon signed-rank test (non-parametric)
            w_stat, w_pval = wilcoxon(scores1, scores2)
            
            comparison_matrix.loc[model1, model2] = f't:{t_pval:.4f}, w:{w_pval:.4f}'
            comparison_matrix.loc[model2, model1] = f't:{t_pval:.4f}, w:{w_pval:.4f}'
        
        # Fill diagonal
        for model in model_names:
            comparison_matrix.loc[model, model] = '1.0000'
        
        # Overall comparison (Friedman test for multiple models)
        if len(model_names) > 2:
            all_scores = [model_results[model]['scores'] for model in model_names]
            friedman_stat, friedman_pval = friedmanchisquare(*all_scores)
            
            print(f"Friedman test statistic: {friedman_stat:.4f}")
            print(f"Friedman test p-value: {friedman_pval:.4f}")
            
            if friedman_pval < alpha:
                print("Significant difference detected between models (Friedman test)")
            else:
                print("No significant difference between models (Friedman test)")
        
        return comparison_matrix
    
    def effect_size_analysis(self, model_results):
        """
        Calculate effect sizes (Cohen's d) for model comparisons
        """
        import itertools
        
        model_names = list(model_results.keys())
        effect_sizes = {}
        
        for model1, model2 in itertools.combinations(model_names, 2):
            scores1 = np.array(model_results[model1]['scores'])
            scores2 = np.array(model_results[model2]['scores'])
            
            # Cohen's d
            pooled_std = np.sqrt(((len(scores1) - 1) * np.var(scores1) + 
                                (len(scores2) - 1) * np.var(scores2)) / 
                               (len(scores1) + len(scores2) - 2))
            
            cohens_d = (np.mean(scores1) - np.mean(scores2)) / pooled_std
            
            effect_sizes[f"{model1}_vs_{model2}"] = {
                'cohens_d': cohens_d,
                'magnitude': self._interpret_effect_size(abs(cohens_d))
            }
        
        return effect_sizes
    
    def _interpret_effect_size(self, d):
        """Interpret Cohen's d effect size"""
        if d < 0.2:
            return "negligible"
        elif d < 0.5:
            return "small"
        elif d < 0.8:
            return "medium"
        else:
            return "large"
    
    def comprehensive_model_comparison(self, models, X, y, cv_strategy, 
                                    scoring_metrics=None, n_repeats=5):
        """
        Comprehensive comparison of multiple models with multiple metrics
        """
        if scoring_metrics is None:
            scoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
        
        results = {}
        
        for model_name, model in models.items():
            print(f"Evaluating {model_name}...")
            model_results = {}
            
            for metric in scoring_metrics:
                metric_scores = []
                
                for repeat in range(n_repeats):
                    # Create fresh CV splitter for each repeat
                    if hasattr(cv_strategy, 'random_state'):
                        cv_strategy.random_state = self.random_state + repeat
                    
                    try:
                        scores = cross_val_score(model, X, y, cv=cv_strategy, 
                                               scoring=metric, n_jobs=-1)
                        metric_scores.extend(scores)
                    except Exception as e:
                        print(f"Error evaluating {model_name} with {metric}: {str(e)}")
                        metric_scores = [0.0] * cv_strategy.n_splits
                
                model_results[metric] = {
                    'scores': metric_scores,
                    'mean': np.mean(metric_scores),
                    'std': np.std(metric_scores),
                    'confidence_interval': self._calculate_confidence_interval(metric_scores)
                }
            
            results[model_name] = model_results
        
        # Statistical comparisons for each metric
        statistical_results = {}
        for metric in scoring_metrics:
            metric_results = {model: results[model][metric] for model in results.keys()}
            statistical_results[metric] = self.statistical_comparison(metric_results)
        
        return results, statistical_results
    
    def _calculate_confidence_interval(self, scores, confidence=0.95):
        """Calculate confidence interval for scores"""
        n = len(scores)
        mean = np.mean(scores)
        std_err = np.std(scores) / np.sqrt(n)
        
        # t-distribution for small samples
        from scipy.stats import t
        t_value = t.ppf((1 + confidence) / 2, df=n-1)
        
        margin_error = t_value * std_err
        return (mean - margin_error, mean + margin_error)
    
    def create_comparison_report(self, results, statistical_results):
        """Create comprehensive comparison report"""
        print("=" * 80)
        print("COMPREHENSIVE MODEL COMPARISON REPORT")
        print("=" * 80)
        
        # Performance summary
        print("\n1. PERFORMANCE SUMMARY")
        print("-" * 40)
        
        for model_name, model_results in results.items():
            print(f"\n{model_name}:")
            for metric, metric_results in model_results.items():
                mean_score = metric_results['mean']
                std_score = metric_results['std']
                ci_lower, ci_upper = metric_results['confidence_interval']
                
                print(f"  {metric:15s}: {mean_score:.4f} ¬± {std_score:.4f} "
                      f"[{ci_lower:.4f}, {ci_upper:.4f}]")
        
        # Statistical significance
        print(f"\n2. STATISTICAL SIGNIFICANCE")
        print("-" * 40)
        
        for metric, comparison_matrix in statistical_results.items():
            print(f"\n{metric.upper()} - Pairwise p-values (t-test, wilcoxon):")
            print(comparison_matrix)
        
        # Recommendations
        print(f"\n3. RECOMMENDATIONS")
        print("-" * 40)
        
        # Find best model for each metric
        best_models = {}
        for metric in results[list(results.keys())[0]].keys():
            best_model = max(results.keys(), 
                           key=lambda x: results[x][metric]['mean'])
            best_score = results[best_model][metric]['mean']
            best_models[metric] = (best_model, best_score)
        
        for metric, (best_model, best_score) in best_models.items():
            print(f"{metric:15s}: {best_model} ({best_score:.4f})")
        
        return best_models
```

## 9.2 Custom Evaluation Metrics and Business-Specific Scoring

```python
class CustomMetrics:
    """Custom evaluation metrics for business-specific objectives"""
    
    @staticmethod
    def profit_based_score(y_true, y_pred, cost_matrix):
        """
        Calculate profit-based score using cost matrix
        
        cost_matrix: dict with keys 'tp', 'fp', 'tn', 'fn' representing
                    profit/cost for each outcome
        """
        from sklearn.metrics import confusion_matrix
        
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        total_profit = (tp * cost_matrix['tp'] + 
                       fp * cost_matrix['fp'] + 
                       tn * cost_matrix['tn'] + 
                       fn * cost_matrix['fn'])
        
        return total_profit
    
    @staticmethod
    def weighted_f1_custom(y_true, y_pred, class_weights):
        """Custom weighted F1 score with business-defined class weights"""
        from sklearn.metrics import precision_recall_fscore_support
        
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None
        )
        
        weighted_f1 = sum(f1[i] * class_weights.get(i, 1.0) for i in range(len(f1)))
        total_weight = sum(class_weights.values())
        
        return weighted_f1 / total_weight
    
    @staticmethod
    def top_k_accuracy(y_true, y_pred_proba, k=3):
        """Calculate top-k accuracy for multi-class problems"""
        top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]
        
        correct = 0
        for i, true_label in enumerate(y_true):
            if true_label in top_k_preds[i]:
                correct += 1
        
        return correct / len(y_true)
    
    @staticmethod
    def regression_within_tolerance(y_true, y_pred, tolerance=0.1):
        """Percentage of predictions within tolerance for regression"""
        relative_errors = np.abs((y_true - y_pred) / y_true)
        return (relative_errors <= tolerance).mean()
    
    @staticmethod
    def business_impact_score(y_true, y_pred, impact_function):
        """
        Generic business impact score using custom impact function
        
        impact_function: function that takes (y_true, y_pred) and returns impact
        """
        return impact_function(y_true, y_pred)

class ImbalancedDatasetEvaluation:
    """Specialized evaluation for imbalanced datasets"""
    
    def __init__(self, positive_class=1):
        self.positive_class = positive_class
    
    def comprehensive_imbalanced_evaluation(self, y_true, y_pred, y_pred_proba=None):
        """Comprehensive evaluation for imbalanced datasets"""
        from sklearn.metrics import (precision_recall_curve, average_precision_score,
                                   roc_curve, auc, confusion_matrix, classification_report)
        
        results = {}
        
        # Basic metrics
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # Calculate metrics manually for clarity
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        results['confusion_matrix'] = cm
        results['precision'] = precision
        results['recall'] = recall
        results['specificity'] = specificity
        results['f1_score'] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # Balanced accuracy
        results['balanced_accuracy'] = (recall + specificity) / 2
        
        # Matthews Correlation Coefficient
        mcc_denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
        results['mcc'] = ((tp * tn) - (fp * fn)) / mcc_denominator if mcc_denominator > 0 else 0
        
        if y_pred_proba is not None:
            # Precision-Recall curve and AUC
            precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_pred_proba)
            results['pr_auc'] = average_precision_score(y_true, y_pred_proba)
            results['pr_curve'] = (precision_curve, recall_curve, pr_thresholds)
            
            # ROC curve and AUC
            fpr, tpr, roc_thresholds = roc_curve(y_true, y_pred_proba)
            results['roc_auc'] = auc(fpr, tpr)
            results['roc_curve'] = (fpr, tpr, roc_thresholds)
        
        return results
    
    def threshold_optimization(self, y_true, y_pred_proba, optimization_metric='f1'):
        """Optimize classification threshold for imbalanced datasets"""
        from sklearn.metrics import precision_recall_curve
        
        precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
        
        if optimization_metric == 'f1':
            # Find threshold that maximizes F1 score
            f1_scores = 2 * (precision * recall) / (precision + recall)
            f1_scores = np.nan_to_num(f1_scores)  # Handle division by zero
            best_threshold_idx = np.argmax(f1_scores)
            best_threshold = thresholds[best_threshold_idx]
            best_score = f1_scores[best_threshold_idx]
        
        elif optimization_metric == 'youden_index':
            # Youden's J statistic: sensitivity + specificity - 1
            fpr, tpr, roc_thresholds = roc_curve(y_true, y_pred_proba)
            youden_index = tpr - fpr
            best_threshold_idx = np.argmax(youden_index)
            best_threshold = roc_thresholds[best_threshold_idx]
            best_score = youden_index[best_threshold_idx]
        
        elif optimization_metric == 'precision_at_recall':
            # Find threshold for specific recall level
            target_recall = 0.8  # Can be parameterized
            valid_indices = recall >= target_recall
            if np.any(valid_indices):
                best_precision_idx = np.argmax(precision[valid_indices])
                actual_idx = np.where(valid_indices)[0][best_precision_idx]
                best_threshold = thresholds[actual_idx]
                best_score = precision[actual_idx]
            else:
                best_threshold = 0.5
                best_score = 0.0
        
        return best_threshold, best_score
    
    def cost_sensitive_evaluation(self, y_true, y_pred_proba, cost_matrix):
        """
        Find optimal threshold based on cost matrix
        
        cost_matrix: dict with 'tp', 'fp', 'tn', 'fn' costs
        """
        thresholds = np.linspace(0.01, 0.99, 100)
        costs = []
        
        for threshold in thresholds:
            y_pred_thresh = (y_pred_proba >= threshold).astype(int)
            cm = confusion_matrix(y_true, y_pred_thresh)
            
            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
                total_cost = (tp * cost_matrix['tp'] + 
                             fp * cost_matrix['fp'] + 
                             tn * cost_matrix['tn'] + 
                             fn * cost_matrix['fn'])
                costs.append(total_cost)
            else:
                costs.append(float('inf'))
        
        best_threshold_idx = np.argmin(costs)
        best_threshold = thresholds[best_threshold_idx]
        best_cost = costs[best_threshold_idx]
        
        return best_threshold, best_cost, thresholds, costs
```

## 9.3 Time Series Model Evaluation

```python
class TimeSeriesEvaluation:
    """Specialized evaluation methods for time series models"""
    
    def __init__(self):
        self.evaluation_results = {}
    
    def walk_forward_validation(self, model, X, y, time_column, 
                               initial_train_size=None, step_size=1, 
                               forecast_horizon=1):
        """
        Implement walk-forward validation for time series
        """
        # Sort by time
        time_sorted = X.sort_values(time_column)
        X_sorted = time_sorted.drop(columns=[time_column])
        y_sorted = y.loc[time_sorted.index]
        
        n_samples = len(X_sorted)
        if initial_train_size is None:
            initial_train_size = n_samples // 3
        
        predictions = []
        actuals = []
        train_sizes = []
        
        for start_idx in range(initial_train_size, 
                             n_samples - forecast_horizon + 1, 
                             step_size):
            
            # Training data: from beginning to current point
            X_train = X_sorted.iloc[:start_idx]
            y_train = y_sorted.iloc[:start_idx]
            
            # Test data: next forecast_horizon points
            X_test = X_sorted.iloc[start_idx:start_idx + forecast_horizon]
            y_test = y_sorted.iloc[start_idx:start_idx + forecast_horizon]
            
            # Train and predict
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            
            predictions.extend(y_pred)
            actuals.extend(y_test.values)
            train_sizes.append(len(X_train))
        
        return np.array(predictions), np.array(actuals), train_sizes
    
    def time_series_cv_with_gap(self, model, X, y, time_column, 
                               n_splits=5, gap_size=0, test_size=None):
        """
        Time series cross-validation with gap between train and test
        """
        # Sort by time
        time_sorted = X.sort_values(time_column)
        X_sorted = time_sorted.drop(columns=[time_column])
        y_sorted = y.loc[time_sorted.index]
        
        n_samples = len(X_sorted)
        if test_size is None:
            test_size = n_samples // (n_splits + 1)
        
        fold_results = []
        
        for i in range(n_splits):
            # Calculate split points
            test_start = (i + 1) * test_size + i * gap_size
            test_end = test_start + test_size
            train_end = test_start - gap_size
            
            if test_end > n_samples or train_end <= 0:
                continue
            
            # Split data
            X_train = X_sorted.iloc[:train_end]
            y_train = y_sorted.iloc[:train_end]
            X_test = X_sorted.iloc[test_start:test_end]
            y_test = y_sorted.iloc[test_start:test_end]
            
            # Train and evaluate
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            
            # Calculate metrics
            fold_metrics = self._calculate_ts_metrics(y_test.values, y_pred)
            fold_results.append(fold_metrics)
        
        return fold_results
    
    def _calculate_ts_metrics(self, y_true, y_pred):
        """Calculate time series specific metrics"""
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        
        metrics = {}
        
        # Standard regression metrics
        metrics['mse'] = mean_squared_error(y_true, y_pred)
        metrics['rmse'] = np.sqrt(metrics['mse'])
        metrics['mae'] = mean_absolute_error(y_true, y_pred)
        
        # Time series specific metrics
        # Mean Absolute Percentage Error
        metrics['mape'] = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        # Symmetric MAPE (handles zero values better)
        metrics['smape'] = np.mean(2 * np.abs(y_pred - y_true) / 
                                 (np.abs(y_true) + np.abs(y_pred))) * 100
        
        # Directional accuracy (for trend prediction)
        if len(y_true) > 1:
            true_direction = np.sign(np.diff(y_true))
            pred_direction = np.sign(np.diff(y_pred))
            metrics['directional_accuracy'] = np.mean(true_direction == pred_direction)
        else:
            metrics['directional_accuracy'] = np.nan
        
        return metrics
    
    def residual_analysis(self, y_true, y_pred, time_index=None):
        """Comprehensive residual analysis for time series"""
        residuals = y_true - y_pred
        
        analysis = {}
        
        # Basic statistics
        analysis['mean_residual'] = np.mean(residuals)
        analysis['std_residual'] = np.std(residuals)
        analysis['skewness'] = stats.skew(residuals)
        analysis['kurtosis'] = stats.kurtosis(residuals)
        
        # Normality test
        _, analysis['normality_pvalue'] = stats.jarque_bera(residuals)
        
        # Autocorrelation test (if time index provided)
        if time_index is not None:
            # Ljung-Box test for autocorrelation
            from statsmodels.stats.diagnostic import acorr_ljungbox
            lb_stat, lb_pvalue = acorr_ljungbox(residuals, lags=min(10, len(residuals)//4))
            analysis['ljung_box_pvalue'] = lb_pvalue.iloc[-1]  # Take last lag p-value
        
        # Heteroscedasticity test
        if len(y_pred) > 10:
            # Breusch-Pagan test
            from scipy.stats import pearsonr
            _, analysis['heteroscedasticity_pvalue'] = pearsonr(np.abs(residuals), y_pred)
        
        return analysis, residuals
    
    def forecast_evaluation_metrics(self, y_true, y_pred, seasonal_period=None):
        """
        Advanced forecast evaluation metrics
        """
        metrics = {}
        
        # Standard metrics
        metrics.update(self._calculate_ts_metrics(y_true, y_pred))
        
        # Forecast skill metrics
        if seasonal_period is not None:
            # Seasonal naive forecast for comparison
            seasonal_naive = np.roll(y_true, seasonal_period)[:len(y_pred)]
            seasonal_naive[:seasonal_period] = y_true[:seasonal_period]  # Fill initial values
            
            naive_mse = mean_squared_error(y_true, seasonal_naive)
            model_mse = mean_squared_error(y_true, y_pred)
            
            # Forecast skill score
            metrics['forecast_skill'] = 1 - (model_mse / naive_mse)
        
        # Prediction interval coverage (if available)
        # This would require prediction intervals from the model
        
        return metrics
```

## 9.4 Automated Model Selection Pipeline

```python
class AutomatedModelSelection:
    """Automated model selection and hyperparameter optimization pipeline"""
    
    def __init__(self, random_state=42, n_jobs=-1):
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.results = {}
        self.best_model = None
        self.selection_history = []
    
    def define_search_space(self, problem_type='classification'):
        """Define comprehensive search space for different problem types"""
        
        if problem_type == 'classification':
            from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
            from sklearn.linear_model import LogisticRegression
            from sklearn.svm import SVC
            from sklearn.naive_bayes import GaussianNB
            import xgboost as xgb
            
            search_space = {
                'logistic_regression': {
                    'model': LogisticRegression(random_state=self.random_state),
                    'params': {
                        'C': [0.001, 0.01, 0.1, 1, 10, 100],
                        'penalty': ['l1', 'l2'],
                        'solver': ['liblinear', 'saga']
                    }
                },
                'random_forest': {
                    'model': RandomForestClassifier(random_state=self.random_state),
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'max_depth': [None, 10, 20, 30],
                        'min_samples_split': [2, 5, 10],
                        'min_samples_leaf': [1, 2, 4]
                    }
                },
                'gradient_boosting': {
                    'model': GradientBoostingClassifier(random_state=self.random_state),
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'learning_rate': [0.01, 0.1, 0.2],
                        'max_depth': [3, 5, 7],
                        'subsample': [0.8, 0.9, 1.0]
                    }
                },
                'xgboost': {
                    'model': xgb.XGBClassifier(random_state=self.random_state),
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'learning_rate': [0.01, 0.1, 0.2],
                        'max_depth': [3, 5, 7],
                        'subsample': [0.8, 0.9, 1.0]
                    }
                }
            }
            
        elif problem_type == 'regression':
            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
            from sklearn.linear_model import LinearRegression, Ridge, Lasso
            from sklearn.svm import SVR
            import xgboost as xgb
            
            search_space = {
                'linear_regression': {
                    'model': LinearRegression(),
                    'params': {}
                },
                'ridge_regression': {
                    'model': Ridge(random_state=self.random_state),
                    'params': {
                        'alpha': [0.001, 0.01, 0.1, 1, 10, 100]
                    }
                },
                'lasso_regression': {
                    'model': Lasso(random_state=self.random_state),
                    'params': {
                        'alpha': [0.001, 0.01, 0.1, 1, 10, 100]
                    }
                },
                'random_forest': {
                    'model': RandomForestRegressor(random_state=self.random_state),
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'max_depth': [None, 10, 20, 30],
                        'min_samples_split': [2, 5, 10]
                    }
                },
                'xgboost': {
                    'model': xgb.XGBRegressor(random_state=self.random_state),
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'learning_rate': [0.01, 0.1, 0.2],
                        'max_depth': [3, 5, 7]
                    }
                }
            }
        
        return search_space
    
    def progressive_model_selection(self, X, y, problem_type='classification', 
                                  cv_strategy=None, scoring=None, 
                                  max_iterations=10, early_stopping_rounds=3):
        """
        Progressive model selection with early stopping
        """
        from sklearn.model_selection import RandomizedSearchCV
        
        if cv_strategy is None:
            cv_strategy = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        if scoring is None:
            scoring = 'accuracy' if problem_type == 'classification' else 'r2'
        
        search_space = self.define_search_space(problem_type)
        
        model_scores = []
        no_improvement_count = 0
        best_score = -np.inf
        
        print("Starting progressive model selection...")
        
        for iteration in range(max_iterations):
            print(f"\nIteration {iteration + 1}/{max_iterations}")
            
            # Select models to evaluate (can implement smart selection here)
            models_to_evaluate = list(search_space.keys())
            
            iteration_results = {}
            
            for model_name in models_to_evaluate:
                model_config = search_space[model_name]
                
                if model_config['params']:
                    # Randomized search for hyperparameters
                    search = RandomizedSearchCV(
                        model_config['model'],
                        model_config['params'],
                        n_iter=20,  # Reduced for progressive approach
                        cv=cv_strategy,
                        scoring=scoring,
                        n_jobs=self.n_jobs,
                        random_state=self.random_state + iteration
                    )
                    
                    search.fit(X, y)
                    best_model = search.best_estimator_
                    best_score_iter = search.best_score_
                    
                else:
                    # No hyperparameters to tune
                    model_config['model'].fit(X, y)
                    scores = cross_val_score(model_config['model'], X, y, 
                                           cv=cv_strategy, scoring=scoring)
                    best_score_iter = scores.mean()
                    best_model = model_config['model']
                
                iteration_results[model_name] = {
                    'score': best_score_iter,
                    'model': best_model
                }
                
                print(f"  {model_name}: {best_score_iter:.4f}")
            
            # Find best model in this iteration
            best_model_name = max(iteration_results.keys(), 
                                key=lambda x: iteration_results[x]['score'])
            iter_best_score = iteration_results[best_model_name]['score']
            
            model_scores.append(iter_best_score)
            
            # Check for improvement
            if iter_best_score > best_score:
                best_score = iter_best_score
                self.best_model = iteration_results[best_model_name]['model']
                no_improvement_count = 0
                print(f"  New best score: {best_score:.4f} ({best_model_name})")
            else:
                no_improvement_count += 1
                print(f"  No improvement for {no_improvement_count} iterations")
            
            # Early stopping
            if no_improvement_count >= early_stopping_rounds:
                print(f"  Early stopping after {early_stopping_rounds} iterations without improvement")
                break
            
            # Update search space based on results (adaptive approach)
            self._update_search_space(search_space, iteration_results)
        
        self.results['progressive_selection'] = {
            'scores_by_iteration': model_scores,
            'best_score': best_score,
            'best_model': self.best_model,
            'total_iterations': iteration + 1
        }
        
        return self.best_model, best_score
    
    def _update_search_space(self, search_space, iteration_results):
        """Update search space based on iteration results (simplified version)"""
        # This is a placeholder for adaptive search space modification
        # In practice, you might:
        # 1. Focus on promising model types
        # 2. Narrow hyperparameter ranges around good values
        # 3. Add new models based on ensemble opportunities
        pass
    
    def bayesian_optimization_selection(self, X, y, problem_type='classification',
                                      cv_strategy=None, n_calls=50):
        """
        Model selection using Bayesian optimization
        Requires scikit-optimize: pip install scikit-optimize
        """
        try:
            from skopt import gp_minimize
            from skopt.space import Real, Integer, Categorical
            from skopt.utils import use_named_args
        except ImportError:
            print("scikit-optimize not available. Install with: pip install scikit-optimize")
            return None, None
        
        if cv_strategy is None:
            cv_strategy = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        # Define search space for Bayesian optimization
        search_dimensions = [
            Categorical(['random_forest', 'gradient_boosting', 'xgboost'], name='model_type'),
            Integer(50, 200, name='n_estimators'),
            Real(0.01, 0.3, name='learning_rate'),
            Integer(3, 10, name='max_depth'),
            Real(0.1, 1.0, name='subsample')
        ]
        
        @use_named_args(search_dimensions)
        def objective(**params):
            # Create model based on parameters
            model_type = params['model_type']
            
            if model_type == 'random_forest':
                from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
                ModelClass = RandomForestClassifier if problem_type == 'classification' else RandomForestRegressor
                model = ModelClass(
                    n_estimators=params['n_estimators'],
                    max_depth=params['max_depth'],
                    random_state=self.random_state
                )
            elif model_type == 'gradient_boosting':
                from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
                ModelClass = GradientBoostingClassifier if problem_type == 'classification' else GradientBoostingRegressor
                model = ModelClass(
                    n_estimators=params['n_estimators'],
                    learning_rate=params['learning_rate'],
                    max_depth=params['max_depth'],
                    subsample=params['subsample'],
                    random_state=self.random_state
                )
            else:  # xgboost
                import xgboost as xgb
                ModelClass = xgb.XGBClassifier if problem_type == 'classification' else xgb.XGBRegressor
                model = ModelClass(
                    n_estimators=params['n_estimators'],
                    learning_rate=params['learning_rate'],
                    max_depth=params['max_depth'],
                    subsample=params['subsample'],
                    random_state=self.random_state
                )
            
            # Evaluate model
            scoring = 'accuracy' if problem_type == 'classification' else 'r2'
            scores = cross_val_score(model, X, y, cv=cv_strategy, scoring=scoring)
            
            # Return negative score for minimization
            return -scores.mean()
        
        # Run Bayesian optimization
        print("Running Bayesian optimization...")
        result = gp_minimize(objective, search_dimensions, n_calls=n_calls, 
                           random_state=self.random_state)
        
        # Extract best parameters and create best model
        best_params = dict(zip([dim.name for dim in search_dimensions], result.x))
        print(f"Best parameters: {best_params}")
        print(f"Best score: {-result.fun:.4f}")
        
        # Create and return best model
        # ... (implementation similar to objective function)
        
        return result, best_params
    
    def ensemble_model_selection(self, X, y, base_models, cv_strategy=None, 
                               ensemble_methods=['voting', 'stacking']):
        """
        Evaluate ensemble methods with selected base models
        """
        from sklearn.ensemble import VotingClassifier, VotingRegressor
        from sklearn.model_selection import cross_val_score
        
        if cv_strategy is None:
            cv_strategy = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        ensemble_results = {}
        
        # Voting ensemble
        if 'voting' in ensemble_methods:
            problem_type = 'classification' if hasattr(base_models[0][1], 'predict_proba') else 'regression'
            
            if problem_type == 'classification':
                voting_ensemble = VotingClassifier(base_models, voting='soft')
                scoring = 'accuracy'
            else:
                voting_ensemble = VotingRegressor(base_models)
                scoring = 'r2'
            
            voting_scores = cross_val_score(voting_ensemble, X, y, cv=cv_strategy, scoring=scoring)
            ensemble_results['voting'] = {
                'scores': voting_scores,
                'mean_score': voting_scores.mean(),
                'std_score': voting_scores.std(),
                'model': voting_ensemble
            }
            
            print(f"Voting ensemble score: {voting_scores.mean():.4f} ¬± {voting_scores.std():.4f}")
        
        # Stacking ensemble
        if 'stacking' in ensemble_methods:
            from sklearn.ensemble import StackingClassifier, StackingRegressor
            from sklearn.linear_model import LogisticRegression, Ridge
            
            problem_type = 'classification' if hasattr(base_models[0][1], 'predict_proba') else 'regression'
            
            if problem_type == 'classification':
                meta_learner = LogisticRegression(random_state=self.random_state)
                stacking_ensemble = StackingClassifier(base_models, final_estimator=meta_learner,
                                                     cv=3, n_jobs=self.n_jobs)
                scoring = 'accuracy'
            else:
                meta_learner = Ridge(random_state=self.random_state)
                stacking_ensemble = StackingRegressor(base_models, final_estimator=meta_learner,
                                                    cv=3, n_jobs=self.n_jobs)
                scoring = 'r2'
            
            stacking_scores = cross_val_score(stacking_ensemble, X, y, cv=cv_strategy, scoring=scoring)
            ensemble_results['stacking'] = {
                'scores': stacking_scores,
                'mean_score': stacking_scores.mean(),
                'std_score': stacking_scores.std(),
                'model': stacking_ensemble
            }
            
            print(f"Stacking ensemble score: {stacking_scores.mean():.4f} ¬± {stacking_scores.std():.4f}")
        
        return ensemble_results
```

## 9.5 Practical Implementation Lab

```python
def comprehensive_model_evaluation_lab():
    """
    Comprehensive lab for advanced model evaluation techniques
    """
    
    print("ADVANCED MODEL EVALUATION LAB")
    print("=" * 50)
    
    # Generate sample dataset for demonstration
    from sklearn.datasets import make_classification
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    
    # Create dataset
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                              n_redundant=5, n_clusters_per_class=1, 
                              class_sep=0.8, random_state=42)
    
    X_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
    y_series = pd.Series(y)
    
    print(f"Dataset created: {X_df.shape[0]} samples, {X_df.shape[1]} features")
    print(f"Class distribution: {pd.Series(y).value_counts().to_dict()}")
    
    # 1. Advanced Cross-Validation
    print("\n1. ADVANCED CROSS-VALIDATION")
    print("-" * 30)
    
    cv_framework = AdvancedCrossValidation()
    
    # Define CV strategies
    cv_strategies = {
        'standard_kfold': KFold(n_splits=5, shuffle=True, random_state=42),
        'stratified_kfold': StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    }
    
    # Evaluate CV stability
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    stability_results = cv_framework.evaluate_cv_stability(
        rf_model, X_df, y_series, cv_strategies, n_repeats=3
    )
    
    print("CV Stability Results:")
    for strategy, results in stability_results.items():
        print(f"  {strategy}: Mean={results['mean']:.4f}, CV={results['cv']:.4f}")
    
    # 2. Nested Cross-Validation
    print("\n2. NESTED CROSS-VALIDATION")
    print("-" * 30)
    
    param_grid = {
        'n_estimators': [50, 100, 150],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    }
    
    nested_results = cv_framework.nested_cross_validation(
        RandomForestClassifier(random_state=42),
        param_grid, X_df, y_series,
        outer_cv=3, inner_cv=3
    )
    
    # 3. Comprehensive Model Comparison
    print("\n3. COMPREHENSIVE MODEL COMPARISON")
    print("-" * 30)
    
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42),
        'SVM': SVC(random_state=42, probability=True)
    }
    
    comparison_framework = ModelComparisonFramework()
    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    results, statistical_results = comparison_framework.comprehensive_model_comparison(
        models, X_df, y_series, cv_strategy,
        scoring_metrics=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],
        n_repeats=2
    )
    
    # Generate comparison report
    best_models = comparison_framework.create_comparison_report(results, statistical_results)
    
    # 4. Custom Business Metrics
    print("\n4. CUSTOM BUSINESS METRICS")
    print("-" * 30)
    
    # Example: Cost-sensitive evaluation
    cost_matrix = {
        'tp': 100,   # Revenue from correctly identifying positive
        'tn': 10,    # Cost savings from correctly identifying negative  
        'fp': -50,   # Cost of false positive
        'fn': -200   # Cost of missing positive
    }
    
    # Train best model and evaluate with custom metric
    best_model = models['Random Forest']
    best_model.fit(X_df, y_series)
    y_pred = best_model.predict(X_df)
    
    profit_score = CustomMetrics.profit_based_score(y_series, y_pred, cost_matrix)
    print(f"Profit-based score: ${profit_score:,.0f}")
    
    # 5. Automated Model Selection
    print("\n5. AUTOMATED MODEL SELECTION")
    print("-" * 30)
    
    auto_selector = AutomatedModelSelection(random_state=42)
    
    # Progressive selection
    best_model_prog, best_score_prog = auto_selector.progressive_model_selection(
        X_df, y_series, problem_type='classification',
        max_iterations=3, early_stopping_rounds=2
    )
    
    print(f"Progressive selection - Best score: {best_score_prog:.4f}")
    
    return {
        'nested_cv_results': nested_results,
        'model_comparison': results,
        'statistical_comparison': statistical_results,
        'best_models': best_models,
        'automated_selection': auto_selector.results
    }

# Run the comprehensive lab
if __name__ == "__main__":
    lab_results = comprehensive_model_evaluation_lab()

## 9.6 Model Evaluation Best Practices and Common Pitfalls

### 9.6.1 Best Practices Checklist

```python
class ModelEvaluationBestPractices:
    """Comprehensive checklist and guidelines for model evaluation"""
    
    @staticmethod
    def evaluation_checklist():
        """Complete evaluation checklist for ML projects"""
        
        checklist = {
            "Data Splitting": [
                "‚úì Hold-out test set never used for model development",
                "‚úì Stratified splitting for imbalanced datasets",
                "‚úì Time-based splitting for temporal data",
                "‚úì Group-aware splitting when necessary",
                "‚úì Consistent random seeds for reproducibility"
            ],
            
            "Cross-Validation": [
                "‚úì Appropriate CV strategy for data type",
                "‚úì Sufficient number of folds (typically 5-10)",
                "‚úì Nested CV for hyperparameter optimization",
                "‚úì Statistical significance testing between models",
                "‚úì Stability analysis across CV folds"
            ],
            
            "Metric Selection": [
                "‚úì Metrics aligned with business objectives",
                "‚úì Multiple complementary metrics used",
                "‚úì Appropriate metrics for class imbalance",
                "‚úì Domain-specific metrics when applicable",
                "‚úì Confidence intervals reported"
            ],
            
            "Model Comparison": [
                "‚úì Statistical significance testing performed",
                "‚úì Effect size analysis conducted",
                "‚úì Computational cost considered",
                "‚úì Interpretability requirements addressed",
                "‚úì Robustness analysis completed"
            ],
            
            "Validation": [
                "‚úì Out-of-time validation for temporal data",
                "‚úì Out-of-sample validation on different populations",
                "‚úì Adversarial testing performed",
                "‚úì Performance monitoring plan established",
                "‚úì Model degradation thresholds defined"
            ]
        }
        
        return checklist
    
    @staticmethod
    def common_pitfalls():
        """Common pitfalls in model evaluation and how to avoid them"""
        
        pitfalls = {
            "Data Leakage": {
                "description": "Information from the future or target leaking into features",
                "examples": [
                    "Using statistics calculated on entire dataset before splitting",
                    "Including features derived from target variable",
                    "Using future information in time series models"
                ],
                "prevention": [
                    "Always split data before any preprocessing",
                    "Careful feature engineering review",
                    "Time-aware validation for temporal data"
                ]
            },
            
            "Overfitting to Validation Set": {
                "description": "Repeated model selection on same validation set",
                "examples": [
                    "Multiple rounds of hyperparameter tuning on same validation set",
                    "Model selection based on validation performance only",
                    "Extensive feature selection using validation performance"
                ],
                "prevention": [
                    "Use nested cross-validation",
                    "Hold-out final test set",
                    "Limit validation set usage"
                ]
            },
            
            "Inappropriate Metrics": {
                "description": "Using metrics not suitable for the problem or business context",
                "examples": [
                    "Using accuracy for highly imbalanced datasets",
                    "Ignoring class costs in business applications",
                    "Single metric evaluation for complex problems"
                ],
                "prevention": [
                    "Understand business context and costs",
                    "Use multiple complementary metrics",
                    "Consider class imbalance and costs"
                ]
            },
            
            "Statistical Issues": {
                "description": "Improper statistical analysis of results",
                "examples": [
                    "Comparing models without significance testing",
                    "Ignoring multiple testing corrections",
                    "Assuming normal distribution of performance metrics"
                ],
                "prevention": [
                    "Use appropriate statistical tests",
                    "Apply multiple testing corrections",
                    "Report confidence intervals"
                ]
            }
        }
        
        return pitfalls
    
    @staticmethod
    def generate_evaluation_report(model_results, test_results, business_context):
        """Generate comprehensive evaluation report template"""
        
        report_template = f"""
        # Model Evaluation Report
        
        ## Executive Summary
        - **Best Model**: {test_results.get('best_model_name', 'TBD')}
        - **Performance**: {test_results.get('best_score', 'TBD'):.4f}
        - **Business Impact**: {business_context.get('expected_impact', 'TBD')}
        - **Recommendation**: {business_context.get('recommendation', 'TBD')}
        
        ## Model Performance Summary
        
        ### Cross-Validation Results
        {ModelEvaluationBestPractices._format_cv_results(model_results)}
        
        ### Test Set Results
        {ModelEvaluationBestPractices._format_test_results(test_results)}
        
        ### Statistical Significance
        - Significance tests performed: Yes/No
        - P-values: [Details]
        - Effect sizes: [Details]
        
        ## Business Context Analysis
        
        ### Performance Requirements
        - Minimum acceptable performance: {business_context.get('min_performance', 'TBD')}
        - Current model meets requirements: Yes/No
        - Performance vs. business metrics alignment: [Analysis]
        
        ### Implementation Considerations
        - Computational requirements: [Details]
        - Interpretability needs: [Assessment]
        - Deployment constraints: [List]
        - Monitoring plan: [Strategy]
        
        ## Risk Assessment
        
        ### Model Risks
        - Overfitting risk: Low/Medium/High
        - Generalization concerns: [Details]
        - Bias/fairness issues: [Assessment]
        - Robustness analysis: [Results]
        
        ### Mitigation Strategies
        - [List of mitigation approaches]
        
        ## Recommendations
        
        ### Model Selection
        - Primary recommendation: [Model + justification]
        - Alternative options: [Backup models]
        - Ensemble considerations: [Analysis]
        
        ### Next Steps
        1. [Action item 1]
        2. [Action item 2]
        3. [Action item 3]
        
        ## Appendices
        
        ### A. Detailed Performance Metrics
        [Comprehensive metrics table]
        
        ### B. Statistical Analysis
        [Detailed statistical results]
        
        ### C. Code and Reproducibility
        [Implementation details and reproduction instructions]
        """
        
        return report_template
    
    @staticmethod
    def _format_cv_results(results):
        """Format cross-validation results for report"""
        # Placeholder - would format actual results
        return "Cross-validation results formatted here"
    
    @staticmethod
    def _format_test_results(results):
        """Format test results for report"""
        # Placeholder - would format actual results  
        return "Test results formatted here"

class PerformanceMonitoringStrategy:
    """Strategy for monitoring model performance in production"""
    
    def __init__(self, model_name, performance_thresholds):
        self.model_name = model_name
        self.performance_thresholds = performance_thresholds
        self.monitoring_history = []
    
    def setup_monitoring_framework(self):
        """Setup comprehensive monitoring framework"""
        
        monitoring_components = {
            "Data Quality Monitoring": {
                "metrics": ["missing_value_rate", "data_drift_score", "feature_distribution_change"],
                "thresholds": {"missing_value_rate": 0.05, "drift_score": 0.1},
                "frequency": "daily"
            },
            
            "Performance Monitoring": {
                "metrics": ["accuracy", "precision", "recall", "f1_score"],
                "thresholds": self.performance_thresholds,
                "frequency": "weekly"
            },
            
            "Business Metrics Monitoring": {
                "metrics": ["conversion_rate", "revenue_impact", "cost_savings"],
                "thresholds": {"conversion_rate": 0.02, "revenue_impact": 0.05},
                "frequency": "monthly"
            },
            
            "Model Behavior Monitoring": {
                "metrics": ["prediction_distribution", "confidence_scores", "feature_importance_drift"],
                "thresholds": {"prediction_drift": 0.1, "confidence_threshold": 0.7},
                "frequency": "daily"
            }
        }
        
        return monitoring_components
    
    def define_alerting_rules(self):
        """Define alerting rules for different scenarios"""
        
        alerting_rules = {
            "Critical Alerts": {
                "triggers": [
                    "Performance drops below minimum threshold",
                    "Data pipeline failure",
                    "Model prediction errors spike"
                ],
                "response_time": "immediate",
                "escalation": "on-call engineer + ML team lead"
            },
            
            "Warning Alerts": {
                "triggers": [
                    "Performance declining trend",
                    "Data drift detected",
                    "Unusual prediction patterns"
                ],
                "response_time": "within 4 hours",
                "escalation": "ML team"
            },
            
            "Info Alerts": {
                "triggers": [
                    "Weekly performance report",
                    "Monthly model review due",
                    "Scheduled retraining recommended"
                ],
                "response_time": "next business day",
                "escalation": "model owner"
            }
        }
        
        return alerting_rules
    
    def retraining_strategy(self):
        """Define model retraining strategy"""
        
        retraining_strategy = {
            "Scheduled Retraining": {
                "frequency": "monthly",
                "triggers": ["calendar_schedule"],
                "validation_required": True
            },
            
            "Performance-Based Retraining": {
                "frequency": "as_needed",
                "triggers": ["performance_degradation", "data_drift"],
                "validation_required": True
            },
            
            "Emergency Retraining": {
                "frequency": "immediate",
                "triggers": ["critical_performance_drop", "data_quality_issues"],
                "validation_required": True,
                "rollback_plan": True
            }
        }
        
        return retraining_strategy
```

---

## 9.7 Chapter Summary

This chapter provided comprehensive coverage of advanced model selection and evaluation techniques essential for building robust, production-ready machine learning systems.

### Key Concepts Covered:

1. **Advanced Cross-Validation Strategies**
   - Specialized CV techniques for different data types
   - Nested cross-validation for unbiased performance estimation
   - Time series and group-aware validation methods
   - Statistical significance testing between models

2. **Custom Evaluation Frameworks**
   - Business-specific metrics and cost-sensitive evaluation
   - Imbalanced dataset evaluation techniques
   - Time series model evaluation methods
   - Custom scoring functions aligned with business objectives

3. **Automated Model Selection**
   - Progressive model selection with early stopping
   - Bayesian optimization for hyperparameter tuning
   - Ensemble method evaluation and selection
   - Comprehensive search space definition

4. **Statistical Model Comparison**
   - Significance testing (t-tests, Wilcoxon, Friedman)
   - Effect size analysis (Cohen's d)
   - Multiple testing corrections
   - Confidence interval estimation

5. **Production Considerations**
   - Performance monitoring strategies
   - Model degradation detection
   - Retraining triggers and strategies
   - Comprehensive evaluation reporting

### Technical Implementation Highlights:

- **Complete code frameworks** for all evaluation techniques
- **Statistical testing implementations** for model comparison
- **Automated selection pipelines** with early stopping
- **Custom metric definitions** for business alignment
- **Monitoring and alerting frameworks** for production deployment

### Best Practices Emphasized:

- Rigorous validation methodology to prevent overfitting
- Statistical significance testing for model comparison
- Business-aligned metric selection and evaluation
- Comprehensive monitoring and maintenance strategies
- Reproducible evaluation procedures

This chapter serves as a comprehensive guide for implementing robust model evaluation processes that ensure reliable model selection and long-term performance in production environments.

---

## Exercises

### Exercise 9.1: Advanced Cross-Validation Implementation
Implement a custom cross-validation strategy for a specific domain (e.g., medical diagnosis with patient groups, financial time series with market regimes). Include:
- Custom splitting logic
- Appropriate evaluation metrics
- Statistical significance testing

### Exercise 9.2: Business-Specific Evaluation Framework
Design a complete evaluation framework for a specific business problem:
- Define custom business metrics
- Implement cost-sensitive evaluation
- Create decision thresholds optimization
- Develop ROI analysis

### Exercise 9.3: Automated Model Selection Pipeline
Build an automated model selection pipeline that includes:
- Progressive model selection with early stopping
- Bayesian optimization for hyperparameter tuning
- Ensemble method evaluation
- Statistical comparison and reporting

### Exercise 9.4: Model Comparison Study
Conduct a comprehensive model comparison study:
- Compare at least 5 different algorithms
- Use multiple evaluation metrics
- Perform statistical significance testing
- Analyze effect sizes and practical significance
- Create detailed comparison report

### Exercise 9.5: Production Monitoring System
Design and implement a production model monitoring system:
- Performance degradation detection
- Data drift monitoring
- Automated alerting rules
- Retraining trigger mechanisms
- Performance dashboard creation
