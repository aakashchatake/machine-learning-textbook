# Chapter 3: Feature Engineering

> "Feature engineering is often the difference between a good model and a great model."
> 
> — Anonymous Data Scientist

## What You'll Learn in This Chapter

By the end of this chapter, you'll master:
- Feature scaling and normalization techniques
- Various feature selection methods
- Feature extraction using PCA and LDA
- Advanced feature engineering strategies
- Feature importance evaluation and interpretation

## The Mathematical Science of Feature Engineering

Feature engineering lies at the heart of statistical learning theory, representing the crucial transformation from raw observations to informative representations that enable effective pattern recognition. From an information-theoretic perspective, the goal is to maximize the mutual information between features and targets while minimizing redundancy.

**Information-Theoretic Foundation**

Consider the fundamental relationship between features X and target Y. The mutual information I(X;Y) quantifies how much knowing X reduces uncertainty about Y:

**I(X;Y) = H(Y) - H(Y|X)**

Where H(Y) is the entropy of Y and H(Y|X) is the conditional entropy. Feature engineering aims to find transformations f(X) that maximize I(f(X);Y).

**The Feature Representation Problem**

In statistical learning, we assume data is generated by some unknown process P(X,Y). The challenge is that raw features X_raw may not provide the optimal representation for learning this relationship. Feature engineering seeks transformations:

**X_engineered = φ(X_raw)**

Such that a learning algorithm can more easily approximate the true function:

**f*: X_engineered → Y**

This is analogous to basis functions in functional analysis—we're finding a representation space where the target function has desirable properties (linearity, smoothness, separability).

Think of features as coordinates in a multi-dimensional space where our learning algorithm searches for patterns. Just as choosing the right coordinate system can make calculus problems trivial or impossible, choosing the right features can make prediction problems learnable or intractable.

## Statistical Evidence for Feature Engineering Impact

Feature engineering fundamentally changes the learning problem's statistical properties. The choice of features affects three critical aspects of model performance: bias, variance, and computational complexity.

**The Bias-Variance-Complexity Trade-off**

Poor features increase model bias by making the true relationship harder to approximate. Rich features can reduce bias but increase variance by providing more ways to overfit. The optimal feature set minimizes:

**Total Error = Bias² + Variance + Irreducible Error + Computational Cost**

**Dimensionality and Sample Complexity**

The curse of dimensionality shows that sample complexity grows exponentially with irrelevant dimensions. If we have d features, we typically need O(2^d) samples to maintain the same generalization performance. Feature engineering helps by:

1. **Reducing effective dimensionality** through feature selection
2. **Increasing signal-to-noise ratio** through feature transformation
3. **Encoding domain knowledge** to guide the learning process

Consider two scenarios with different feature representations:

**Scenario 1: Raw Features**
```python
# Raw customer data
customer_data = {
    'purchase_date': '2023-05-15',
    'birth_date': '1990-03-22',
    'purchase_amount': 150.75,
    'last_purchase': '2023-04-10'
}
```

**Scenario 2: Engineered Features**
```python
# Engineered features from the same data
engineered_features = {
    'customer_age': 33,
    'days_since_last_purchase': 35,
    'purchase_amount_log': 5.015,
    'is_weekend_purchase': True,
    'purchase_frequency': 2.4  # purchases per month
}
```

The engineered features provide much more predictive power because they capture relationships and patterns that raw data doesn't reveal directly.

## Feature Scaling: Mathematical Foundations

### The Mathematical Scale Problem

Feature scaling addresses fundamental mathematical issues in optimization and distance computation. Many algorithms implicitly assume that all features contribute equally to similarity measures or gradient computations.

**Distance-Based Algorithm Bias**

Consider the Euclidean distance between two points in feature space:

**d(x₁, x₂) = √(Σᵢ (x₁ᵢ - x₂ᵢ)²)**

Without scaling, features with larger numerical ranges dominate this calculation. For a dataset with features of scales [1, 1000], the second feature contributes 10⁶ times more to distance calculations than the first, regardless of predictive importance.

**Gradient-Based Optimization Issues**

In gradient descent, the update rule is:

**θᵢ ← θᵢ - α ∇_θᵢ J(θ)**

When features have different scales, the gradients ∇_θᵢ have different magnitudes. This creates an ill-conditioned optimization problem where:
- Parameters for large-scale features have small gradients (slow learning)
- Parameters for small-scale features have large gradients (unstable learning)

**Condition Number and Convergence**

The condition number κ of a matrix measures how difficult the optimization problem is. For unscaled features:

**κ = λ_max / λ_min**

Where λ are eigenvalues of the Hessian matrix. Poor scaling leads to high condition numbers, requiring more iterations for convergence and increasing numerical instability.

Machine learning algorithms often struggle when features have vastly different scales. Consider this dataset:

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# Example: House features with different scales
house_data = pd.DataFrame({
    'price': [250000, 300000, 180000, 450000, 320000],
    'sqft': [1200, 1500, 900, 2200, 1600],
    'bedrooms': [2, 3, 2, 4, 3],
    'age_years': [10, 5, 25, 2, 8]
})

print("Original data ranges:")
print(house_data.describe())
```

The price ranges from 180,000 to 450,000, while bedrooms only range from 2 to 4. This scale difference can cause problems for algorithms like KNN or neural networks.

### Standardization (Z-Score Normalization)

Standardization transforms features to have mean = 0 and standard deviation = 1.

**Formula**: `z = (x - μ) / σ`

```python
def demonstrate_standardization():
    """Show standardization in action"""
    
    # Apply standardization
    scaler = StandardScaler()
    standardized_data = scaler.fit_transform(house_data)
    
    # Convert back to DataFrame for readability
    standardized_df = pd.DataFrame(
        standardized_data, 
        columns=house_data.columns
    )
    
    print("After standardization:")
    print(standardized_df.round(3))
    print(f"\nMeans: {standardized_df.mean().round(3)}")
    print(f"Standard deviations: {standardized_df.std().round(3)}")
    
    return standardized_df

# Example output shows all features centered around 0
```

**When to use**: Most algorithms (SVM, Neural Networks, PCA) when you want to preserve the shape of the distribution.

### Min-Max Scaling

Scales features to a fixed range, typically [0, 1].

**Formula**: `x_scaled = (x - x_min) / (x_max - x_min)`

```python
def demonstrate_minmax_scaling():
    """Show Min-Max scaling in action"""
    
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(house_data)
    
    scaled_df = pd.DataFrame(scaled_data, columns=house_data.columns)
    
    print("After Min-Max scaling:")
    print(scaled_df.round(3))
    print(f"\nMinimums: {scaled_df.min()}")
    print(f"Maximums: {scaled_df.max()}")
    
    return scaled_df

# All features now range from 0 to 1
```

**When to use**: When you know the approximate upper and lower bounds of your data, or when you need a specific range.

### Robust Scaling

Uses median and interquartile range, less sensitive to outliers.

**Formula**: `x_scaled = (x - median) / IQR`

```python
def demonstrate_robust_scaling():
    """Show Robust scaling with outliers"""
    
    # Add outlier to demonstrate robustness
    data_with_outlier = house_data.copy()
    data_with_outlier.loc[5] = [1000000, 1400, 3, 15]  # Expensive outlier
    
    print("Data with outlier:")
    print(data_with_outlier)
    
    # Compare Standard vs Robust scaling
    standard_scaler = StandardScaler()
    robust_scaler = RobustScaler()
    
    standard_scaled = standard_scaler.fit_transform(data_with_outlier)
    robust_scaled = robust_scaler.fit_transform(data_with_outlier)
    
    print("\nStandard scaling (affected by outlier):")
    print(pd.DataFrame(standard_scaled, columns=house_data.columns).round(3))
    
    print("\nRobust scaling (less affected by outlier):")
    print(pd.DataFrame(robust_scaled, columns=house_data.columns).round(3))

# Robust scaling handles outliers better
```

**When to use**: When your data contains outliers that you want to preserve but not let dominate the scaling.

### Scaling Comparison Visualization

```python
import matplotlib.pyplot as plt

def visualize_scaling_methods():
    """Compare different scaling methods visually"""
    
    # Generate sample data with different distributions
    np.random.seed(42)
    data = pd.DataFrame({
        'normal': np.random.normal(100, 15, 1000),
        'exponential': np.random.exponential(2, 1000),
        'uniform': np.random.uniform(0, 50, 1000)
    })
    
    # Apply different scaling methods
    scalers = {
        'Original': None,
        'StandardScaler': StandardScaler(),
        'MinMaxScaler': MinMaxScaler(),
        'RobustScaler': RobustScaler()
    }
    
    fig, axes = plt.subplots(4, 3, figsize=(15, 12))
    
    for i, (scaler_name, scaler) in enumerate(scalers.items()):
        if scaler is None:
            scaled_data = data
        else:
            scaled_data = pd.DataFrame(
                scaler.fit_transform(data), 
                columns=data.columns
            )
        
        for j, column in enumerate(data.columns):
            axes[i, j].hist(scaled_data[column], bins=50, alpha=0.7)
            axes[i, j].set_title(f'{scaler_name} - {column}')
            axes[i, j].set_xlabel('Value')
            axes[i, j].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# This shows how each method affects different distributions
```

## Feature Selection: Information Theory and Statistical Foundations

Feature selection addresses the fundamental challenge of identifying which variables carry genuine predictive signal versus those that contribute only noise or redundancy. This process is grounded in information theory, statistical inference, and computational complexity theory.

**Information-Theoretic Perspective**

From an information theory standpoint, we seek features that maximize mutual information with the target while minimizing redundancy among themselves:

**Objective: max I(X_selected; Y) - λ × Redundancy(X_selected)**

Where:
- **I(X_selected; Y)** measures predictive information
- **Redundancy(X_selected)** penalizes correlated features  
- **λ** controls the trade-off between relevance and redundancy

**The Curse of Dimensionality: Mathematical Analysis**

High-dimensional spaces exhibit counterintuitive properties that hurt learning:

1. **Volume Concentration**: In d dimensions, most volume lies near the surface of hyperspheres
2. **Distance Concentration**: All pairwise distances become similar as d → ∞  
3. **Sample Sparsity**: Data becomes exponentially sparse, requiring O(e^d) samples

**Statistical Learning Theory of Feature Selection**

The generalization bound for a model with d features is approximately:

**R(h) ≤ R̂(h) + O(√(d log(n)/n))**

Where R(h) is true risk, R̂(h) is empirical risk, and n is sample size. This shows that excess features directly worsen generalization bounds.

**Three Categories of Feature Selection**

1. **Filter Methods**: Use statistical measures independent of the learning algorithm
2. **Wrapper Methods**: Use the learning algorithm itself to evaluate feature subsets  
3. **Embedded Methods**: Feature selection is built into the learning algorithm

Each approach represents different trade-offs between computational cost and optimality.

### Why Feature Selection Matters

```python
def demonstrate_curse_of_dimensionality():
    """Show how irrelevant features hurt performance"""
    
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score
    
    # Create dataset with increasing numbers of irrelevant features
    n_relevant = 5
    irrelevant_features = [0, 10, 50, 100, 500]
    results = []
    
    for n_irrelevant in irrelevant_features:
        # Generate data
        X, y = make_classification(
            n_samples=1000,
            n_features=n_relevant + n_irrelevant,
            n_informative=n_relevant,
            n_redundant=0,
            n_clusters_per_class=1,
            random_state=42
        )
        
        # Train and evaluate
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        clf = RandomForestClassifier(n_estimators=100, random_state=42)
        clf.fit(X_train, y_train)
        
        accuracy = accuracy_score(y_test, clf.predict(X_test))
        results.append((n_relevant + n_irrelevant, accuracy))
        
        print(f"Total features: {n_relevant + n_irrelevant:3d}, Accuracy: {accuracy:.3f}")
    
    return results

# Shows how adding irrelevant features decreases performance
```

### Filter Methods: Statistical Independence Testing

Filter methods apply statistical hypothesis tests to measure the strength of association between features and targets. These methods are computationally efficient because they evaluate each feature independently of the learning algorithm.

**Mathematical Foundation**

Filter methods test the null hypothesis:
**H₀: Feature X_i is independent of target Y**

The p-value p_i measures the probability of observing the data under H₀. Features with p_i < α (significance threshold) are considered relevant.

**Common Statistical Tests for Filter Methods**:

1. **Pearson Correlation**: Tests linear relationships (continuous variables)
   - **Test statistic**: r = Σ(x-μₓ)(y-μᵧ) / √[Σ(x-μₓ)²Σ(y-μᵧ)²]
   - **Assumptions**: Normal distributions, linear relationships

2. **Chi-Square Test**: Tests independence (categorical variables)
   - **Test statistic**: χ² = Σ(O_ij - E_ij)² / E_ij
   - **Degrees of freedom**: (rows-1) × (columns-1)

3. **ANOVA F-test**: Tests group differences (categorical X, continuous Y)
   - **Test statistic**: F = MSB/MSW (between/within group variance)

4. **Mutual Information**: Measures non-linear dependencies
   - **Formula**: I(X;Y) = Σₓ Σᵧ p(x,y) log(p(x,y)/(p(x)p(y)))

Filter methods provide fast, model-agnostic feature relevance assessment based on univariate statistical relationships.

#### Correlation-Based Selection

```python
def correlation_feature_selection(data, target, threshold=0.7):
    """Select features based on correlation with target and among themselves"""
    
    # Calculate correlation with target
    target_corr = data.corrwith(target).abs().sort_values(ascending=False)
    
    print("Correlation with target:")
    print(target_corr)
    
    # Remove highly correlated features (multicollinearity)
    corr_matrix = data.corr().abs()
    
    # Find pairs of highly correlated features
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if corr_matrix.iloc[i, j] > threshold:
                colname_i = corr_matrix.columns[i]
                colname_j = corr_matrix.columns[j]
                high_corr_pairs.append((colname_i, colname_j, corr_matrix.iloc[i, j]))
    
    print(f"\nHighly correlated pairs (>{threshold}):")
    for pair in high_corr_pairs:
        print(f"{pair[0]} - {pair[1]}: {pair[2]:.3f}")
    
    return target_corr, high_corr_pairs

# Example usage with house data
# target_corr, high_corr = correlation_feature_selection(house_data, target_prices)
```

#### Chi-Square Test for Categorical Features

```python
from sklearn.feature_selection import chi2, SelectKBest

def chi_square_selection(X_categorical, y, k=5):
    """Select categorical features using Chi-square test"""
    
    # Apply Chi-square test
    chi2_selector = SelectKBest(chi2, k=k)
    X_selected = chi2_selector.fit_transform(X_categorical, y)
    
    # Get feature scores
    feature_scores = chi2_selector.scores_
    selected_features = chi2_selector.get_support(indices=True)
    
    print("Chi-square scores:")
    for i, score in enumerate(feature_scores):
        status = "✓" if i in selected_features else "✗"
        print(f"Feature {i}: {score:.3f} {status}")
    
    return X_selected, selected_features

# Example with categorical data
def create_categorical_example():
    """Create example categorical data"""
    
    np.random.seed(42)
    data = pd.DataFrame({
        'color': np.random.choice(['red', 'blue', 'green'], 1000),
        'size': np.random.choice(['small', 'medium', 'large'], 1000),
        'material': np.random.choice(['wood', 'metal', 'plastic'], 1000),
        'brand': np.random.choice(['A', 'B', 'C', 'D'], 1000)
    })
    
    # Create target that depends on some features
    target = (
        (data['color'] == 'red').astype(int) + 
        (data['size'] == 'large').astype(int) + 
        np.random.randint(0, 2, 1000)  # Add noise
    ) > 1
    
    return data, target
```

### Wrapper Methods: Search Theory and Optimization

Wrapper methods treat feature selection as a discrete optimization problem, searching through the exponential space of feature subsets to find the combination that optimizes model performance.

**Mathematical Formulation**

The wrapper method optimization problem is:

**S* = arg max_{S⊆F} CV_score(Algorithm(X_S, y))**

Where:
- **S** is a feature subset from the full feature set F
- **X_S** contains only features in subset S  
- **CV_score** is cross-validation performance
- The search space has **2^|F|** possible subsets

**Computational Complexity**

Exhaustive search has exponential complexity O(2^d), making it intractable for large feature sets. Practical wrapper methods use heuristic search algorithms:

1. **Forward Selection**: Greedy algorithm with O(d²) evaluations
2. **Backward Elimination**: Greedy algorithm with O(d²) evaluations  
3. **Bidirectional Search**: Combines forward/backward with O(d²) evaluations

**Search Strategy Analysis**

**Forward Selection Algorithm**:
1. Start with empty feature set: S = ∅
2. At each step, add feature f that maximizes: CV_score(S ∪ {f})
3. Stop when performance plateaus or max features reached

**Optimality Properties**:
- **Not globally optimal**: Greedy choices may miss better combinations
- **Monotone submodular approximation**: Under certain conditions, achieves (1-1/e) of optimal
- **Local search guarantee**: Finds local optimum in polynomial time

Wrapper methods are computationally expensive but model-specific, often yielding better performance than filter methods by accounting for feature interactions.

#### Forward Selection

```python
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression

def forward_selection_demo(X, y, max_features=5):
    """Demonstrate forward feature selection"""
    
    # Create base estimator
    estimator = LogisticRegression(random_state=42, max_iter=1000)
    
    # Forward selection
    forward_selector = SequentialFeatureSelector(
        estimator,
        n_features_to_select=max_features,
        direction='forward',
        cv=5,
        scoring='accuracy'
    )
    
    # Fit and transform
    X_selected = forward_selector.fit_transform(X, y)
    selected_features = forward_selector.get_support(indices=True)
    
    print("Forward Selection Results:")
    print(f"Selected {len(selected_features)} features: {selected_features}")
    print(f"Original shape: {X.shape}, Selected shape: {X_selected.shape}")
    
    # Show selection process
    print("\nFeature selection scores:")
    for i, selected in enumerate(forward_selector.get_support()):
        status = "✓ Selected" if selected else "✗ Not selected"
        print(f"Feature {i}: {status}")
    
    return X_selected, selected_features

# Manual implementation for educational purposes
def manual_forward_selection(X, y, max_features=5):
    """Manual implementation to understand the process"""
    
    from sklearn.model_selection import cross_val_score
    
    n_features = X.shape[1]
    selected_features = []
    remaining_features = list(range(n_features))
    
    print("Forward Selection Process:")
    print("=" * 50)
    
    for step in range(max_features):
        best_score = 0
        best_feature = None
        
        # Try adding each remaining feature
        for feature in remaining_features:
            current_features = selected_features + [feature]
            X_subset = X.iloc[:, current_features]
            
            # Cross-validate
            estimator = LogisticRegression(random_state=42, max_iter=1000)
            scores = cross_val_score(estimator, X_subset, y, cv=3)
            avg_score = scores.mean()
            
            if avg_score > best_score:
                best_score = avg_score
                best_feature = feature
        
        # Add best feature
        if best_feature is not None:
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
            
            print(f"Step {step + 1}: Added feature {best_feature}, Score: {best_score:.4f}")
    
    return selected_features
```

#### Backward Elimination

```python
def backward_elimination_demo(X, y, min_features=3):
    """Demonstrate backward feature elimination"""
    
    estimator = LogisticRegression(random_state=42, max_iter=1000)
    
    # Backward elimination
    backward_selector = SequentialFeatureSelector(
        estimator,
        n_features_to_select=min_features,
        direction='backward',
        cv=5,
        scoring='accuracy'
    )
    
    X_selected = backward_selector.fit_transform(X, y)
    selected_features = backward_selector.get_support(indices=True)
    
    print("Backward Elimination Results:")
    print(f"Kept {len(selected_features)} features: {selected_features}")
    print(f"Original shape: {X.shape}, Final shape: {X_selected.shape}")
    
    return X_selected, selected_features

def manual_backward_elimination(X, y, min_features=3):
    """Manual implementation of backward elimination"""
    
    from sklearn.model_selection import cross_val_score
    
    current_features = list(range(X.shape[1]))
    
    print("Backward Elimination Process:")
    print("=" * 50)
    
    while len(current_features) > min_features:
        worst_score = float('inf')
        worst_feature = None
        
        # Try removing each feature
        for feature in current_features:
            temp_features = [f for f in current_features if f != feature]
            X_subset = X.iloc[:, temp_features]
            
            estimator = LogisticRegression(random_state=42, max_iter=1000)
            scores = cross_val_score(estimator, X_subset, y, cv=3)
            avg_score = scores.mean()
            
            # We want the removal that gives the best score (least impact)
            if avg_score > worst_score:
                worst_score = avg_score
                worst_feature = feature
        
        # Remove worst feature
        if worst_feature is not None:
            current_features.remove(worst_feature)
            print(f"Removed feature {worst_feature}, Remaining score: {worst_score:.4f}")
    
    return current_features
```

#### Recursive Feature Elimination (RFE)

```python
from sklearn.feature_selection import RFE, RFECV

def rfe_demonstration(X, y, n_features=5):
    """Demonstrate Recursive Feature Elimination"""
    
    # Basic RFE
    estimator = LogisticRegression(random_state=42, max_iter=1000)
    rfe = RFE(estimator, n_features_to_select=n_features)
    
    X_rfe = rfe.fit_transform(X, y)
    
    print("RFE Results:")
    print(f"Selected {n_features} features")
    print("Feature rankings:")
    for i, (rank, support) in enumerate(zip(rfe.ranking_, rfe.support_)):
        status = "✓ Selected" if support else f"✗ Rank {rank}"
        print(f"Feature {i}: {status}")
    
    return X_rfe, rfe.support_

def rfecv_demonstration(X, y):
    """RFE with Cross-Validation to find optimal number of features"""
    
    estimator = LogisticRegression(random_state=42, max_iter=1000)
    rfecv = RFECV(estimator, cv=5, scoring='accuracy')
    
    X_rfecv = rfecv.fit_transform(X, y)
    
    print("RFECV Results:")
    print(f"Optimal number of features: {rfecv.n_features_}")
    print(f"Selected features: {np.where(rfecv.support_)[0]}")
    
    # Plot validation scores
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), 
             rfecv.cv_results_['mean_test_score'], 'o-')
    plt.xlabel('Number of Features')
    plt.ylabel('Cross-Validation Score')
    plt.title('RFE with Cross-Validation')
    plt.axvline(x=rfecv.n_features_, color='red', linestyle='--', 
                label=f'Optimal: {rfecv.n_features_} features')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    return X_rfecv, rfecv.support_
```

### Embedded Methods: Regularization and Sparsity Theory

Embedded methods integrate feature selection directly into the learning objective through regularization, automatically identifying relevant features during optimization. This approach is grounded in sparsity theory and convex optimization.

**Mathematical Foundation: Regularized Optimization**

Embedded methods modify the learning objective by adding a regularization term that encourages sparsity:

**min_θ L(θ; X, y) + λR(θ)**

Where:
- **L(θ; X, y)** is the loss function (e.g., MSE, log-likelihood)
- **R(θ)** is the regularization penalty  
- **λ** controls the sparsity-accuracy trade-off

**L1 Regularization (Lasso): Mathematical Properties**

The L1 penalty R(θ) = ||θ||₁ = Σᵢ|θᵢ| has unique theoretical properties:

1. **Sparsity Induction**: L1 penalty drives coefficients exactly to zero
2. **Convex Optimization**: Despite non-differentiability at zero, remains convex
3. **Feature Selection**: Non-zero coefficients correspond to selected features

**Geometric Interpretation**: The L1 constraint forms an L1-ball (diamond in 2D, hyperdiamond in higher dimensions) with corners on coordinate axes, encouraging sparse solutions.

**Statistical Properties of Lasso**

Under certain conditions (restricted eigenvalue condition), Lasso achieves:

**||θ̂ - θ*||₂ ≤ C√(s log(p)/n)**

Where s is the sparsity level and p is the number of features. This bound shows Lasso can handle high-dimensional problems when the true model is sparse.

Embedded methods elegantly solve feature selection and parameter estimation simultaneously within a single optimization framework.

#### Lasso Regularization (L1)

```python
from sklearn.linear_model import LassoCV
from sklearn.feature_selection import SelectFromModel

def lasso_feature_selection(X, y):
    """Use Lasso regularization for feature selection"""
    
    # Find optimal alpha using cross-validation
    lasso_cv = LassoCV(cv=5, random_state=42, max_iter=1000)
    lasso_cv.fit(X, y)
    
    print("Lasso Feature Selection:")
    print(f"Optimal alpha: {lasso_cv.alpha_:.6f}")
    
    # Show coefficients
    feature_importance = pd.DataFrame({
        'feature': range(len(lasso_cv.coef_)),
        'coefficient': lasso_cv.coef_
    }).sort_values('coefficient', key=abs, ascending=False)
    
    print("\nFeature coefficients (sorted by absolute value):")
    print(feature_importance)
    
    # Select features with non-zero coefficients
    selector = SelectFromModel(lasso_cv, prefit=True)
    X_selected = selector.transform(X)
    selected_features = selector.get_support(indices=True)
    
    print(f"\nSelected {len(selected_features)} features with non-zero coefficients")
    print(f"Selected feature indices: {selected_features}")
    
    # Visualize coefficients
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.bar(range(len(lasso_cv.coef_)), lasso_cv.coef_)
    plt.xlabel('Feature Index')
    plt.ylabel('Coefficient Value')
    plt.title('Lasso Coefficients')
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    
    plt.subplot(1, 2, 2)
    selected_coef = lasso_cv.coef_[selected_features]
    plt.barh(range(len(selected_coef)), selected_coef)
    plt.xlabel('Selected Feature Index')
    plt.ylabel('Coefficient Value')
    plt.title('Selected Features Coefficients')
    
    plt.tight_layout()
    plt.show()
    
    return X_selected, selected_features, lasso_cv.coef_

def lasso_path_visualization(X, y):
    """Visualize how coefficients change with regularization strength"""
    
    from sklearn.linear_model import lasso_path
    
    alphas, coefs, _ = lasso_path(X, y, max_iter=1000)
    
    plt.figure(figsize=(12, 8))
    plt.plot(alphas, coefs.T)
    plt.xlabel('Alpha (Regularization Strength)')
    plt.ylabel('Coefficient Value')
    plt.title('Lasso Path - How Coefficients Change with Regularization')
    plt.xscale('log')
    plt.grid(True, alpha=0.3)
    
    # Add vertical line for optimal alpha
    lasso_cv = LassoCV(cv=5, random_state=42, max_iter=1000)
    lasso_cv.fit(X, y)
    plt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', 
                label=f'Optimal α = {lasso_cv.alpha_:.4f}')
    plt.legend()
    plt.show()
    
    return alphas, coefs
```

#### Tree-Based Feature Importance

```python
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier

def tree_based_feature_selection(X, y, method='random_forest'):
    """Use tree-based methods for feature importance"""
    
    if method == 'random_forest':
        estimator = RandomForestClassifier(n_estimators=100, random_state=42)
        name = "Random Forest"
    elif method == 'extra_trees':
        estimator = ExtraTreesClassifier(n_estimators=100, random_state=42)
        name = "Extra Trees"
    else:
        estimator = DecisionTreeClassifier(random_state=42)
        name = "Decision Tree"
    
    # Fit and get feature importances
    estimator.fit(X, y)
    importances = estimator.feature_importances_
    
    # Create importance DataFrame
    importance_df = pd.DataFrame({
        'feature': range(len(importances)),
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    print(f"{name} Feature Importance:")
    print(importance_df)
    
    # Select top features
    selector = SelectFromModel(estimator, prefit=True)
    X_selected = selector.transform(X)
    selected_features = selector.get_support(indices=True)
    
    print(f"\nSelected {len(selected_features)} important features")
    
    # Visualize importance
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.bar(range(len(importances)), importances)
    plt.xlabel('Feature Index')
    plt.ylabel('Importance')
    plt.title(f'{name} - All Features')
    
    plt.subplot(1, 2, 2)
    top_features = importance_df.head(10)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), 
               [f'Feature {i}' for i in top_features['feature']])
    plt.xlabel('Importance')
    plt.title(f'{name} - Top 10 Features')
    
    plt.tight_layout()
    plt.show()
    
    return X_selected, selected_features, importances

def feature_importance_comparison(X, y):
    """Compare feature importance across different methods"""
    
    methods = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),
        'Decision Tree': DecisionTreeClassifier(random_state=42)
    }
    
    importance_comparison = pd.DataFrame(index=range(X.shape[1]))
    
    for name, estimator in methods.items():
        estimator.fit(X, y)
        importance_comparison[name] = estimator.feature_importances_
    
    print("Feature Importance Comparison:")
    print(importance_comparison.round(4))
    
    # Plot comparison
    plt.figure(figsize=(12, 8))
    importance_comparison.plot(kind='bar', ax=plt.gca())
    plt.xlabel('Feature Index')
    plt.ylabel('Importance')
    plt.title('Feature Importance Comparison Across Methods')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    return importance_comparison
```

## Feature Extraction Techniques

Feature extraction creates new features from existing ones, often reducing dimensionality while preserving important information. Unlike feature selection, which chooses from existing features, extraction transforms the original features into a new feature space.

### Principal Component Analysis: Linear Algebra and Optimization Theory

PCA is fundamentally an eigenvalue problem that finds the optimal linear transformation for dimensionality reduction. It solves a constrained optimization problem to find directions of maximum variance in the data.

**Mathematical Foundation: The Optimization Problem**

PCA seeks to find orthonormal directions w₁, w₂, ..., wₖ that maximize the variance of projected data:

**max_{w₁,...,wₖ} Σᵢ Var(Xwᵢ) subject to ||wᵢ|| = 1, wᵢᵀwⱼ = 0 for i ≠ j**

This is equivalent to finding eigenvectors of the covariance matrix C = (1/n)XᵀX.

**Eigenvalue Decomposition Solution**

The solution comes from the spectral theorem. For symmetric matrix C:

**C = QΛQᵀ**

Where:
- **Q** contains eigenvectors (principal components)  
- **Λ** contains eigenvalues (variance explained by each component)
- Components are ordered by decreasing eigenvalue: λ₁ ≥ λ₂ ≥ ... ≥ λₚ

**Variance Preservation**

The k-dimensional PCA projection preserves fraction of total variance:

**Variance Ratio = (λ₁ + λ₂ + ... + λₖ) / (λ₁ + λ₂ + ... + λₚ)**

**Optimality Properties**

PCA is optimal in several senses:
1. **Maximum variance**: Maximizes variance of projected data
2. **Minimum reconstruction error**: Minimizes ||X - X̂||²F among all rank-k approximations  
3. **Maximum likelihood**: Under Gaussian assumptions, PCA is the ML solution

The mathematical elegance of PCA lies in connecting variance maximization, error minimization, and eigenvalue decomposition into a unified framework.

#### Mathematical Foundation

```python
import numpy as np
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris, load_digits
import matplotlib.pyplot as plt

def pca_mathematical_explanation():
    """Explain PCA step by step mathematically"""
    
    # Create simple 2D example
    np.random.seed(42)
    
    # Generate correlated data
    mean = [0, 0]
    cov = [[3, 2], [2, 2]]  # Covariance matrix
    data = np.random.multivariate_normal(mean, cov, 200)
    
    print("PCA Step-by-Step:")
    print("=" * 50)
    
    # Step 1: Center the data
    data_centered = data - np.mean(data, axis=0)
    print(f"Step 1 - Data centered: Mean = {np.mean(data_centered, axis=0)}")
    
    # Step 2: Compute covariance matrix
    cov_matrix = np.cov(data_centered.T)
    print(f"Step 2 - Covariance matrix:\n{cov_matrix}")
    
    # Step 3: Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
    
    # Sort by eigenvalue (descending)
    idx = eigenvalues.argsort()[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    print(f"Step 3 - Eigenvalues: {eigenvalues}")
    print(f"Step 3 - Eigenvectors:\n{eigenvectors}")
    
    # Step 4: Transform data
    pca_data = data_centered.dot(eigenvectors)
    
    print(f"Step 4 - Explained variance ratio: {eigenvalues / np.sum(eigenvalues)}")
    
    # Visualize
    plt.figure(figsize=(15, 5))
    
    # Original data
    plt.subplot(1, 3, 1)
    plt.scatter(data[:, 0], data[:, 1], alpha=0.6)
    plt.title('Original Data')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.grid(True, alpha=0.3)
    
    # Centered data with principal components
    plt.subplot(1, 3, 2)
    plt.scatter(data_centered[:, 0], data_centered[:, 1], alpha=0.6)
    
    # Draw principal components
    origin = np.array([0, 0])
    plt.quiver(*origin, eigenvectors[0, 0], eigenvectors[1, 0], 
               scale=1, scale_units='xy', angles='xy', color='red', width=0.005, label='PC1')
    plt.quiver(*origin, eigenvectors[0, 1], eigenvectors[1, 1], 
               scale=1, scale_units='xy', angles='xy', color='blue', width=0.005, label='PC2')
    
    plt.title('Centered Data with Principal Components')
    plt.xlabel('Feature 1 (centered)')
    plt.ylabel('Feature 2 (centered)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    
    # Transformed data
    plt.subplot(1, 3, 3)
    plt.scatter(pca_data[:, 0], pca_data[:, 1], alpha=0.6)
    plt.title('Data in PCA Space')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return data, pca_data, eigenvalues, eigenvectors

def pca_iris_example():
    """Comprehensive PCA example with Iris dataset"""
    
    # Load Iris dataset
    iris = load_iris()
    X = iris.data
    y = iris.target
    
    print("PCA on Iris Dataset:")
    print("=" * 30)
    print(f"Original shape: {X.shape}")
    print(f"Features: {iris.feature_names}")
    
    # Apply PCA
    pca = PCA()
    X_pca = pca.fit_transform(X)
    
    # Analyze components
    print(f"\nExplained variance ratio: {pca.explained_variance_ratio_}")
    print(f"Cumulative explained variance: {np.cumsum(pca.explained_variance_ratio_)}")
    
    # Component interpretation
    components_df = pd.DataFrame(
        pca.components_.T,
        columns=[f'PC{i+1}' for i in range(len(pca.components_))],
        index=iris.feature_names
    )
    
    print(f"\nPrincipal Components (loadings):")
    print(components_df.round(3))
    
    # Visualization
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Scree plot
    axes[0, 0].bar(range(1, len(pca.explained_variance_ratio_) + 1), 
                   pca.explained_variance_ratio_)
    axes[0, 0].plot(range(1, len(pca.explained_variance_ratio_) + 1), 
                    np.cumsum(pca.explained_variance_ratio_), 'ro-')
    axes[0, 0].set_xlabel('Principal Component')
    axes[0, 0].set_ylabel('Explained Variance Ratio')
    axes[0, 0].set_title('Scree Plot')
    axes[0, 0].legend(['Cumulative', 'Individual'])
    axes[0, 0].grid(True, alpha=0.3)
    
    # PC1 vs PC2
    scatter = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
    axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
    axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
    axes[0, 1].set_title('First Two Principal Components')
    plt.colorbar(scatter, ax=axes[0, 1])
    
    # PC1 vs PC3
    scatter = axes[0, 2].scatter(X_pca[:, 0], X_pca[:, 2], c=y, cmap='viridis')
    axes[0, 2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
    axes[0, 2].set_ylabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%} variance)')
    axes[0, 2].set_title('PC1 vs PC3')
    plt.colorbar(scatter, ax=axes[0, 2])
    
    # Component loadings heatmap
    im = axes[1, 0].imshow(pca.components_, cmap='RdBu', aspect='auto')
    axes[1, 0].set_xticks(range(len(iris.feature_names)))
    axes[1, 0].set_xticklabels(iris.feature_names, rotation=45)
    axes[1, 0].set_yticks(range(len(pca.components_)))
    axes[1, 0].set_yticklabels([f'PC{i+1}' for i in range(len(pca.components_))])
    axes[1, 0].set_title('Component Loadings Heatmap')
    plt.colorbar(im, ax=axes[1, 0])
    
    # Individual feature contributions to PC1
    pc1_contributions = np.abs(pca.components_[0])
    axes[1, 1].barh(iris.feature_names, pc1_contributions)
    axes[1, 1].set_xlabel('Absolute Loading')
    axes[1, 1].set_title('Feature Contributions to PC1')
    
    # 3D plot if possible
    if len(pca.components_) >= 3:
        ax = fig.add_subplot(2, 3, 6, projection='3d')
        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis')
        ax.set_xlabel('PC1')
        ax.set_ylabel('PC2')
        ax.set_zlabel('PC3')
        ax.set_title('3D PCA Visualization')
    
    plt.tight_layout()
    plt.show()
    
    return X_pca, pca

def pca_dimensionality_reduction_analysis():
    """Analyze how much dimensionality reduction we can achieve"""
    
    # Use digits dataset for high-dimensional example
    digits = load_digits()
    X = digits.data  # 64 features (8x8 pixel images)
    y = digits.target
    
    print("Dimensionality Reduction Analysis:")
    print("=" * 40)
    print(f"Original dimensions: {X.shape}")
    
    # Apply PCA
    pca = PCA()
    X_pca = pca.fit_transform(X)
    
    # Find number of components for different variance thresholds
    cumsum_var = np.cumsum(pca.explained_variance_ratio_)
    
    thresholds = [0.8, 0.9, 0.95, 0.99]
    for threshold in thresholds:
        n_components = np.argmax(cumsum_var >= threshold) + 1
        compression_ratio = (X.shape[1] - n_components) / X.shape[1] * 100
        print(f"{threshold:.0%} variance: {n_components} components "
              f"({compression_ratio:.1f}% reduction)")
    
    # Visualize
    plt.figure(figsize=(15, 5))
    
    # Cumulative explained variance
    plt.subplot(1, 3, 1)
    plt.plot(range(1, len(cumsum_var) + 1), cumsum_var, 'b-')
    for threshold in thresholds:
        n_comp = np.argmax(cumsum_var >= threshold) + 1
        plt.axhline(y=threshold, color='red', linestyle='--', alpha=0.7)
        plt.axvline(x=n_comp, color='red', linestyle='--', alpha=0.7)
        plt.plot(n_comp, threshold, 'ro')
    
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Cumulative Explained Variance')
    plt.grid(True, alpha=0.3)
    
    # First few original images
    plt.subplot(1, 3, 2)
    sample_images = X[:16].reshape(16, 8, 8)
    combined_image = np.zeros((4*8, 4*8))
    for i in range(4):
        for j in range(4):
            combined_image[i*8:(i+1)*8, j*8:(j+1)*8] = sample_images[i*4 + j]
    
    plt.imshow(combined_image, cmap='gray')
    plt.title('Original Images (64D)')
    plt.axis('off')
    
    # Reconstructed images using reduced dimensions
    plt.subplot(1, 3, 3)
    n_components_reduced = np.argmax(cumsum_var >= 0.9) + 1
    pca_reduced = PCA(n_components=n_components_reduced)
    X_reduced = pca_reduced.fit_transform(X)
    X_reconstructed = pca_reduced.inverse_transform(X_reduced)
    
    reconstructed_images = X_reconstructed[:16].reshape(16, 8, 8)
    combined_reconstructed = np.zeros((4*8, 4*8))
    for i in range(4):
        for j in range(4):
            combined_reconstructed[i*8:(i+1)*8, j*8:(j+1)*8] = reconstructed_images[i*4 + j]
    
    plt.imshow(combined_reconstructed, cmap='gray')
    plt.title(f'Reconstructed ({n_components_reduced}D → 64D)')
    plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return cumsum_var, n_components_reduced
```

#### Linear Discriminant Analysis: Supervised Optimization Theory

LDA extends PCA to supervised dimensionality reduction by incorporating class information. Instead of maximizing total variance like PCA, LDA maximizes the ratio of between-class to within-class variance.

**Mathematical Objective: Fisher's Criterion**

LDA solves the generalized eigenvalue problem to find projection directions that maximize:

**J(w) = (wᵀS_Bw) / (wᵀS_Ww)**

Where:
- **S_B** = between-class scatter matrix = Σᵢ nᵢ(μᵢ - μ)(μᵢ - μ)ᵀ
- **S_W** = within-class scatter matrix = Σᵢ Σₓ∈Cᵢ (x - μᵢ)(x - μᵢ)ᵀ  
- **nᵢ** = number of samples in class i
- **μᵢ** = mean of class i, **μ** = overall mean

**Generalized Eigenvalue Solution**

The optimal projection directions are eigenvectors of S_W⁻¹S_B:

**S_W⁻¹S_B w = λw**

The eigenvalues λ represent the discriminative power of each direction.

**Dimensionality Constraints**

LDA can find at most min(d, C-1) meaningful components, where:
- **d** = original feature dimensionality  
- **C** = number of classes

This limitation arises because S_B has rank at most C-1.

**Optimality Properties**

1. **Maximum Class Separability**: Optimal for linear classification under Gaussian assumptions
2. **Minimum Bayes Error**: Under equal covariances, minimizes classification error
3. **Maximum Likelihood**: Optimal projection for Gaussian class-conditional distributions

LDA provides supervised dimensionality reduction specifically designed for classification tasks.

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

def lda_explanation_and_demo():
    """Explain and demonstrate LDA for supervised dimensionality reduction"""
    
    print("Linear Discriminant Analysis (LDA):")
    print("=" * 40)
    print("LDA vs PCA:")
    print("- PCA: Unsupervised, maximizes variance")
    print("- LDA: Supervised, maximizes class separability")
    
    # Load Iris for comparison
    iris = load_iris()
    X = iris.data
    y = iris.target
    
    # Apply both PCA and LDA
    pca = PCA(n_components=2)
    lda = LinearDiscriminantAnalysis(n_components=2)
    
    X_pca = pca.fit_transform(X)
    X_lda = lda.fit_transform(X, y)
    
    # Compare results
    plt.figure(figsize=(15, 5))
    
    # Original data (first two features)
    plt.subplot(1, 3, 1)
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
    plt.xlabel(iris.feature_names[0])
    plt.ylabel(iris.feature_names[1])
    plt.title('Original Features')
    plt.colorbar(scatter)
    
    # PCA projection
    plt.subplot(1, 3, 2)
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')
    plt.title('PCA Projection')
    plt.colorbar(scatter)
    
    # LDA projection
    plt.subplot(1, 3, 3)
    scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', alpha=0.7)
    plt.xlabel(f'LD1 ({lda.explained_variance_ratio_[0]:.1%} var)')
    plt.ylabel(f'LD2 ({lda.explained_variance_ratio_[1]:.1%} var)')
    plt.title('LDA Projection')
    plt.colorbar(scatter)
    
    plt.tight_layout()
    plt.show()
    
    # Performance comparison
    from sklearn.model_selection import cross_val_score
    from sklearn.naive_bayes import GaussianNB
    
    classifier = GaussianNB()
    
    scores_original = cross_val_score(classifier, X, y, cv=5)
    scores_pca = cross_val_score(classifier, X_pca, y, cv=5)
    scores_lda = cross_val_score(classifier, X_lda, y, cv=5)
    
    print(f"\nClassification Performance:")
    print(f"Original features: {scores_original.mean():.3f} ± {scores_original.std():.3f}")
    print(f"PCA (2D): {scores_pca.mean():.3f} ± {scores_pca.std():.3f}")
    print(f"LDA (2D): {scores_lda.mean():.3f} ± {scores_lda.std():.3f}")
    
    return X_pca, X_lda, pca, lda

def lda_mathematical_insight():
    """Show the mathematical insight behind LDA"""
    
    # Generate synthetic data for clear demonstration
    np.random.seed(42)
    
    # Class 1: centered at (1, 1)
    class1 = np.random.multivariate_normal([1, 1], [[0.3, 0.1], [0.1, 0.3]], 50)
    # Class 2: centered at (3, 2)  
    class2 = np.random.multivariate_normal([3, 2], [[0.3, -0.1], [-0.1, 0.3]], 50)
    # Class 3: centered at (1.5, 3)
    class3 = np.random.multivariate_normal([1.5, 3], [[0.4, 0.0], [0.0, 0.2]], 50)
    
    X = np.vstack([class1, class2, class3])
    y = np.hstack([np.zeros(50), np.ones(50), np.full(50, 2)])
    
    # Apply LDA
    lda = LinearDiscriminantAnalysis()
    X_lda = lda.fit_transform(X, y)
    
    # Calculate within-class and between-class scatter
    def calculate_scatter_matrices(X, y):
        """Calculate within-class and between-class scatter matrices"""
        
        classes = np.unique(y)
        n_features = X.shape[1]
        
        # Overall mean
        mean_overall = np.mean(X, axis=0)
        
        # Within-class scatter matrix
        S_W = np.zeros((n_features, n_features))
        
        # Between-class scatter matrix  
        S_B = np.zeros((n_features, n_features))
        
        for c in classes:
            X_c = X[y == c]
            mean_c = np.mean(X_c, axis=0)
            n_c = X_c.shape[0]
            
            # Within-class scatter
            S_W += np.cov(X_c.T) * (n_c - 1)
            
            # Between-class scatter
            mean_diff = (mean_c - mean_overall).reshape(-1, 1)
            S_B += n_c * (mean_diff @ mean_diff.T)
        
        return S_W, S_B, mean_overall
    
    S_W, S_B, mean_overall = calculate_scatter_matrices(X, y)
    
    print("LDA Mathematical Components:")
    print("=" * 35)
    print(f"Within-class scatter matrix S_W:\n{S_W.round(3)}")
    print(f"\nBetween-class scatter matrix S_B:\n{S_B.round(3)}")
    
    # LDA seeks to maximize: (w^T * S_B * w) / (w^T * S_W * w)
    # This is solved by finding eigenvectors of S_W^(-1) * S_B
    
    try:
        eigenvals, eigenvecs = np.linalg.eig(np.linalg.inv(S_W) @ S_B)
        idx = eigenvals.argsort()[::-1]
        eigenvals = eigenvals[idx]
        eigenvecs = eigenvecs[:, idx]
        
        print(f"\nEigenvalues: {eigenvals.real.round(3)}")
        print(f"LDA directions (eigenvectors):\n{eigenvecs.real.round(3)}")
    
    except np.linalg.LinAlgError:
        print("\nSingular matrix encountered - using pseudoinverse")
    
    # Visualize the separability
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    colors = ['red', 'blue', 'green']
    for i, color in enumerate(colors):
        mask = y == i
        plt.scatter(X[mask, 0], X[mask, 1], c=color, alpha=0.7, label=f'Class {i}')
    
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2') 
    plt.title('Original 2D Data')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    for i, color in enumerate(colors):
        mask = y == i
        plt.scatter(X_lda[mask, 0], X_lda[mask, 1], c=color, alpha=0.7, label=f'Class {i}')
    
    plt.xlabel('Linear Discriminant 1')
    plt.ylabel('Linear Discriminant 2')
    plt.title('LDA Projection - Maximized Separability')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return X_lda, S_W, S_B
```

## 3.4 Advanced Feature Extraction

### Mutual Information: Information-Theoretic Feature Selection

Mutual information provides a principled, information-theoretic approach to measuring feature relevance. Unlike correlation, which only captures linear relationships, mutual information detects any statistical dependency between variables.

**Information-Theoretic Foundation**

Mutual information quantifies the reduction in uncertainty about Y when X is observed:

**I(X;Y) = H(Y) - H(Y|X)**

Where:
- **H(Y) = -Σᵧ p(y) log p(y)** is the entropy of Y (uncertainty before observing X)
- **H(Y|X) = -Σₓ,ᵧ p(x,y) log p(y|x)** is conditional entropy (uncertainty after observing X)

**Alternative Formulation: Kullback-Leibler Divergence**

Mutual information can be expressed as the KL divergence between joint and product distributions:

**I(X;Y) = KL(P(X,Y) || P(X)P(Y)) = Σₓ,ᵧ p(x,y) log [p(x,y) / (p(x)p(y))]**

This formulation highlights that MI measures how much the joint distribution deviates from independence.

**Key Properties**

1. **Symmetry**: I(X;Y) = I(Y;X)  
2. **Non-negativity**: I(X;Y) ≥ 0, with equality iff X and Y are independent
3. **Upper bound**: I(X;Y) ≤ min(H(X), H(Y))
4. **Chain rule**: I(X,Y;Z) = I(X;Z) + I(Y;Z|X)

**Continuous Variable Extension**

For continuous variables, MI uses differential entropy:

**I(X;Y) = ∫∫ p(x,y) log [p(x,y) / (p(x)p(y))] dx dy**

In practice, this requires density estimation or discretization techniques.

#### Implementation

```python
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
from sklearn.datasets import load_breast_cancer
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Calculate mutual information for classification
mi_scores = mutual_info_classif(X, y, random_state=42)

# Create feature importance plot
feature_names = data.feature_names
mi_df = pd.DataFrame({
    'feature': feature_names,
    'mutual_info': mi_scores
}).sort_values('mutual_info', ascending=False)

plt.figure(figsize=(12, 8))
plt.barh(range(len(mi_df.head(15))), mi_df.head(15)['mutual_info'])
plt.yticks(range(len(mi_df.head(15))), mi_df.head(15)['feature'])
plt.xlabel('Mutual Information Score')
plt.title('Top 15 Features by Mutual Information')
plt.tight_layout()
plt.show()

print("Top 10 features by mutual information:")
for i, (feature, score) in enumerate(mi_df.head(10).values):
    print(f"{i+1:2d}. {feature:<25}: {score:.4f}")
```

#### Advantages and Limitations

**Advantages:**
- Captures non-linear relationships
- Model-agnostic
- No assumptions about data distribution

**Limitations:**
- Computationally expensive for large datasets
- Sensitive to discretization for continuous variables
- May not capture complex interactions

### ANOVA F-Test: Statistical Significance Testing

Analysis of Variance (ANOVA) provides a statistical framework for testing whether features show significant differences across groups, making it valuable for feature selection in classification problems.

**Mathematical Foundation: F-Statistic**

The ANOVA F-test compares between-group variance to within-group variance:

**F = MSB / MSW = (SSB/(k-1)) / (SSW/(N-k))**

Where:
- **SSB** = Sum of Squares Between groups = Σᵢ nᵢ(x̄ᵢ - x̄)²  
- **SSW** = Sum of Squares Within groups = ΣᵢΣⱼ(xᵢⱼ - x̄ᵢ)²
- **k** = number of groups, **N** = total sample size

**Statistical Interpretation**

Under null hypothesis H₀: μ₁ = μ₂ = ... = μₖ (all group means equal), F follows F-distribution with (k-1, N-k) degrees of freedom. Large F-values indicate significant group differences, suggesting the feature is informative for classification.

**Feature Selection via ANOVA**

Features with F-statistic exceeding critical value F_α are selected:
**F_computed > F_α(k-1, N-k)**

### Recursive Feature Elimination: Iterative Optimization

RFE implements a greedy backward elimination algorithm that iteratively removes the least important features according to a base estimator.

**Algorithm: Backward Greedy Search**

1. **Initialize**: Start with all features F = {f₁, f₂, ..., fₚ}
2. **Iterate**: While |F| > target_size:
   - Train model on current feature set F
   - Rank features by importance scores
   - Remove lowest-ranked feature: F ← F \ {f_worst}
3. **Output**: Final feature subset F*

**RFE with Cross-Validation (RFECV)**

RFECV extends RFE by using cross-validation to determine optimal feature count:

**n* = arg max_{n∈{1,2,...,p}} CV_score(RFE_n(F))**

This addresses RFE's limitation of requiring pre-specified target dimensionality.

**Theoretical Properties**

- **Computational Complexity**: O(p²) model training calls
- **Optimality**: No global optimality guarantees (greedy heuristic)  
- **Model Dependency**: Results depend heavily on base estimator choice
- **Interaction Handling**: Can capture feature interactions through model training

### Tree-Based Feature Importance: Information Gain Analysis

Tree-based models provide natural feature importance measures through impurity reduction calculations.

**Gini Importance**

For random forests, feature importance is the average decrease in Gini impurity:

**Importance(fⱼ) = (1/B) Σᵦ Σₜ∈T_b p(t) Δi(t,fⱼ)**

Where:
- **B** = number of trees, **T_b** = nodes in tree b
- **p(t)** = proportion of samples reaching node t  
- **Δi(t,fⱼ)** = impurity decrease when splitting on feature fⱼ at node t

### 3.4.2 SHAP (SHapley Additive exPlanations)

SHAP values provide a unified framework for interpreting model predictions by quantifying the contribution of each feature to the prediction.

#### Mathematical Foundation

SHAP values are based on cooperative game theory. For a prediction $f(x)$, the SHAP value $\phi_i$ for feature $i$ is:

$$\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!}[f(S \cup \{i\}) - f(S)]$$

Where:
- $N$ is the set of all features
- $S$ is a subset of features not including $i$
- $f(S)$ is the model prediction using only features in subset $S$

#### Implementation

```python
import shap
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Prepare data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train a model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Calculate SHAP values
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Feature importance plot
shap.summary_plot(shap_values[1], X_test, feature_names=feature_names, 
                  plot_type="bar", show=False)
plt.title('Feature Importance (SHAP Values)')
plt.tight_layout()
plt.show()

# Detailed SHAP summary plot
shap.summary_plot(shap_values[1], X_test, feature_names=feature_names, show=False)
plt.title('SHAP Summary Plot - Feature Impact on Predictions')
plt.tight_layout()
plt.show()

# Calculate mean absolute SHAP values for feature ranking
mean_shap = np.abs(shap_values[1]).mean(axis=0)
shap_df = pd.DataFrame({
    'feature': feature_names,
    'mean_shap': mean_shap
}).sort_values('mean_shap', ascending=False)

print("Top 10 features by SHAP importance:")
for i, (feature, importance) in enumerate(shap_df.head(10).values):
    print(f"{i+1:2d}. {feature:<25}: {importance:.4f}")
```

#### SHAP Waterfall Plot

```python
# Waterfall plot for a single prediction
shap.waterfall_plot(
    explainer.expected_value[1], 
    shap_values[1][0], 
    X_test[0], 
    feature_names=feature_names,
    show=False
)
plt.title('SHAP Waterfall Plot - Individual Prediction Explanation')
plt.tight_layout()
plt.show()
```

### 3.4.3 Comparison of Feature Importance Methods

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import f_classif

# Calculate different types of feature importance
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Tree-based importance
tree_importance = rf_model.feature_importances_

# Statistical test (F-score)
f_scores, _ = f_classif(X_train, y_train)
f_scores_norm = f_scores / f_scores.max()

# Mutual information (already calculated)
mi_scores_norm = mi_scores / mi_scores.max()

# SHAP importance (already calculated)
shap_importance_norm = mean_shap / mean_shap.max()

# Create comparison dataframe
comparison_df = pd.DataFrame({
    'feature': feature_names,
    'tree_importance': tree_importance,
    'f_score': f_scores_norm,
    'mutual_info': mi_scores_norm,
    'shap_importance': shap_importance_norm
})

# Plot comparison
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
methods = ['tree_importance', 'f_score', 'mutual_info', 'shap_importance']
titles = ['Tree-based Importance', 'F-Score', 'Mutual Information', 'SHAP Importance']

for idx, (method, title) in enumerate(zip(methods, titles)):
    ax = axes[idx//2, idx%2]
    top_features = comparison_df.nlargest(15, method)
    ax.barh(range(len(top_features)), top_features[method])
    ax.set_yticks(range(len(top_features)))
    ax.set_yticklabels(top_features['feature'], fontsize=8)
    ax.set_xlabel('Normalized Importance')
    ax.set_title(title)

plt.tight_layout()
plt.show()

# Correlation between different importance measures
correlation_matrix = comparison_df[methods].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Between Different Feature Importance Methods')
plt.tight_layout()
plt.show()
```

## 3.5 Practical Case Studies

### 3.5.1 Case Study: Customer Churn Prediction

Let's apply comprehensive feature engineering to a customer churn prediction problem.

```python
# Simulate customer churn dataset
np.random.seed(42)
n_customers = 1000

# Generate synthetic customer data
customer_data = {
    'tenure': np.random.exponential(24, n_customers),
    'monthly_charges': np.random.normal(65, 20, n_customers),
    'total_charges': np.random.normal(2000, 800, n_customers),
    'age': np.random.normal(40, 15, n_customers),
    'contract_length': np.random.choice([1, 12, 24], n_customers),
    'payment_method': np.random.choice(['credit_card', 'bank_transfer', 'electronic_check'], n_customers),
    'service_calls': np.random.poisson(2, n_customers),
    'data_usage_gb': np.random.exponential(15, n_customers)
}

# Create target variable (churn) with realistic relationships
churn_prob = (
    0.1 + 
    0.3 * (customer_data['service_calls'] > 5).astype(int) +
    0.2 * (customer_data['tenure'] < 6).astype(int) +
    0.15 * (customer_data['monthly_charges'] > 80).astype(int)
)
churn = np.random.binomial(1, np.clip(churn_prob, 0, 1), n_customers)

# Create DataFrame
df_churn = pd.DataFrame(customer_data)
df_churn['churn'] = churn

print("Dataset shape:", df_churn.shape)
print("\nChurn distribution:")
print(df_churn['churn'].value_counts(normalize=True))
```

#### Feature Engineering Pipeline

```python
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Define feature engineering steps
def create_feature_engineering_pipeline():
    # Numerical features
    numerical_features = ['tenure', 'monthly_charges', 'total_charges', 'age', 'data_usage_gb']
    
    # Categorical features
    categorical_features = ['contract_length', 'payment_method']
    
    # Create new features
    df_churn['avg_monthly_usage'] = df_churn['data_usage_gb'] / df_churn['tenure']
    df_churn['charges_per_gb'] = df_churn['monthly_charges'] / (df_churn['data_usage_gb'] + 0.1)
    df_churn['high_service_calls'] = (df_churn['service_calls'] > 3).astype(int)
    df_churn['new_customer'] = (df_churn['tenure'] < 12).astype(int)
    
    # Binning
    df_churn['age_group'] = pd.cut(df_churn['age'], 
                                   bins=[0, 25, 35, 50, 100], 
                                   labels=['young', 'adult', 'middle_aged', 'senior'])
    
    return df_churn

# Apply feature engineering
df_engineered = create_feature_engineering_pipeline()

print("New features created:")
new_features = ['avg_monthly_usage', 'charges_per_gb', 'high_service_calls', 'new_customer', 'age_group']
for feature in new_features:
    print(f"- {feature}")
```

#### Model Evaluation with Feature Engineering

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Prepare features for modeling
X = df_engineered.drop(['churn'], axis=1)
y = df_engineered['churn']

# Encode categorical variables
le_payment = LabelEncoder()
le_age_group = LabelEncoder()

X_encoded = X.copy()
X_encoded['payment_method'] = le_payment.fit_transform(X['payment_method'])
X_encoded['age_group'] = le_age_group.fit_transform(X['age_group'])

# Scale numerical features
scaler = StandardScaler()
numerical_cols = ['tenure', 'monthly_charges', 'total_charges', 'age', 'data_usage_gb', 
                  'avg_monthly_usage', 'charges_per_gb']
X_encoded[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])

# Compare models with and without feature engineering
X_original = df_churn[['tenure', 'monthly_charges', 'total_charges', 'age', 'service_calls', 'data_usage_gb']]
X_original_scaled = scaler.fit_transform(X_original)

# Train models
models = {
    'Random Forest (Original)': RandomForestClassifier(random_state=42),
    'Random Forest (Engineered)': RandomForestClassifier(random_state=42),
    'Logistic Regression (Original)': LogisticRegression(random_state=42),
    'Logistic Regression (Engineered)': LogisticRegression(random_state=42)
}

datasets = {
    'Random Forest (Original)': X_original_scaled,
    'Random Forest (Engineered)': X_encoded,
    'Logistic Regression (Original)': X_original_scaled,
    'Logistic Regression (Engineered)': X_encoded
}

print("Model Performance Comparison:")
print("-" * 50)
for model_name, model in models.items():
    X_data = datasets[model_name]
    scores = cross_val_score(model, X_data, y, cv=5, scoring='accuracy')
    print(f"{model_name:<30}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

## 3.6 Best Practices and Guidelines

### 3.6.1 Feature Engineering Best Practices

1. **Domain Knowledge First**
   - Understand the business context
   - Consult with domain experts
   - Research existing literature

2. **Start Simple**
   - Begin with basic transformations
   - Gradually add complexity
   - Validate each step

3. **Avoid Data Leakage**
   - Never use future information
   - Be careful with target-derived features
   - Apply transformations properly in cross-validation

4. **Handle Missing Values Appropriately**
   - Understand why data is missing
   - Choose appropriate imputation strategies
   - Consider missingness as information

5. **Scale and Transform Consistently**
   - Fit transformations on training data only
   - Apply same transformations to test data
   - Use pipelines for reproducibility

### 3.6.2 Common Pitfalls to Avoid

```python
# Example: Proper way to handle feature engineering in cross-validation
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif

# WRONG: Fitting scaler on entire dataset
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)  # Data leakage!
# scores = cross_val_score(model, X_scaled, y, cv=5)

# CORRECT: Using pipeline to prevent data leakage
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('selector', SelectKBest(f_classif, k=10)),
    ('classifier', LogisticRegression())
])

scores = cross_val_score(pipeline, X, y, cv=5)
print(f"Cross-validation accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

### 3.6.3 Feature Engineering Checklist

- [ ] **Understand the data**: Explore distributions, correlations, missing values
- [ ] **Domain research**: Investigate domain-specific transformations
- [ ] **Handle missing values**: Choose appropriate imputation strategy
- [ ] **Encode categoricals**: Use appropriate encoding method
- [ ] **Scale features**: Apply scaling when needed
- [ ] **Create interactions**: Generate meaningful feature combinations
- [ ] **Engineer temporal features**: Extract time-based patterns
- [ ] **Apply dimensionality reduction**: When dealing with high dimensions
- [ ] **Validate transformations**: Check for data leakage and proper splits
- [ ] **Document process**: Keep track of all transformations

## Theoretical and Practical Synthesis

**1. Information-Theoretic Foundation**: Feature engineering maximizes mutual information I(X;Y) between features and targets while minimizing redundancy, providing a principled approach to representation learning.

**2. Mathematical Necessity of Scaling**: Feature scaling addresses fundamental issues in optimization and distance computation, preventing scale-dependent biases and improving convergence properties of learning algorithms.

**3. Statistical Learning Theory**: The curse of dimensionality shows that generalization bounds worsen with irrelevant features, making feature selection mathematically essential for good performance.

**4. Optimization Perspectives on Selection Methods**:
   - **Filter methods**: Efficient univariate statistical tests (O(p) complexity)
   - **Wrapper methods**: Exponential search problem solved with greedy heuristics  
   - **Embedded methods**: Sparsity-inducing regularization integrates selection into learning

**5. Linear Algebra Foundations of Extraction**:
   - **PCA**: Eigenvalue decomposition optimizing variance preservation
   - **LDA**: Generalized eigenvalue problem optimizing class separability
   - Both provide mathematically optimal solutions to their respective objectives

**6. Statistical Validation**: All feature engineering must respect train-test independence to maintain valid generalization estimates and avoid optimistic bias in performance evaluation.

## 3.8 Exercises

### Exercise 3.1: Feature Scaling Comparison
Load the Wine dataset and compare the performance of different scaling methods on a logistic regression classifier. Use cross-validation to get robust estimates.

### Exercise 3.2: Feature Selection Pipeline
Create a complete feature selection pipeline for the Breast Cancer dataset that:
1. Applies univariate selection (SelectKBest)
2. Uses recursive feature elimination
3. Compares results with tree-based feature importance
4. Evaluates the impact on model performance

### Exercise 3.3: PCA Analysis
Perform PCA on the Digits dataset and:
1. Plot the explained variance ratio
2. Determine how many components explain 95% of variance
3. Visualize the first two principal components
4. Compare classification accuracy with different numbers of components

### Exercise 3.4: Advanced Feature Engineering
Using the Boston Housing dataset:
1. Create polynomial features of degree 2
2. Apply different scaling methods
3. Use mutual information for feature selection
4. Compare model performance before and after feature engineering

### Exercise 3.5: Real-world Application
Choose a dataset from your domain of interest and:
1. Perform comprehensive exploratory data analysis
2. Apply appropriate feature engineering techniques
3. Use multiple feature selection methods
4. Document your process and justify your choices
5. Evaluate the impact on model performance

---

*This completes Chapter 3: Feature Engineering. The next chapter will cover Classification Algorithms, building on the feature engineering techniques learned here.*
