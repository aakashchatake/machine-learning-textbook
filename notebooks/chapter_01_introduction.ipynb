{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8612e4",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Machine Learning\n",
    "## Interactive Notebook\n",
    "\n",
    "Welcome to the hands-on companion notebook for Chapter 1! This notebook demonstrates key concepts through practical examples.\n",
    "\n",
    "### What You'll Do in This Notebook:\n",
    "1. **Compare Traditional vs ML Programming** - See the difference in action\n",
    "2. **Explore Supervised Learning** - Classification and regression examples\n",
    "3. **Discover Unsupervised Learning** - Clustering and dimensionality reduction\n",
    "4. **Try Reinforcement Learning** - Simple agent-environment interaction\n",
    "5. **Master Python ML Libraries** - NumPy, Pandas, Matplotlib, Scikit-learn\n",
    "\n",
    "Let's begin our machine learning journey! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f60070",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Let's start by importing all the Python libraries we'll need for our machine learning examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Machine Learning Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_boston, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Import sklearn explicitly for version check\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364a5ec",
   "metadata": {},
   "source": [
    "## Section 2: Traditional Programming vs Machine Learning\n",
    "\n",
    "Let's see the fundamental difference between traditional programming and machine learning approaches with a practical example: **Email Spam Detection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea359afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Programming Approach: Rule-based Spam Detection\n",
    "def is_spam_traditional(email_text):\n",
    "    \"\"\"Traditional approach using hand-crafted rules\"\"\"\n",
    "    spam_score = 0\n",
    "    email_lower = email_text.lower()\n",
    "    \n",
    "    # Rule 1: Check for suspicious words\n",
    "    spam_words = ['free', 'money', 'winner', 'urgent', 'act now', 'limited time']\n",
    "    for word in spam_words:\n",
    "        if word in email_lower:\n",
    "            spam_score += 1\n",
    "    \n",
    "    # Rule 2: Check for excessive punctuation\n",
    "    if email_text.count('!') > 3:\n",
    "        spam_score += 1\n",
    "    \n",
    "    # Rule 3: Check for all caps\n",
    "    caps_ratio = sum(1 for c in email_text if c.isupper()) / len(email_text)\n",
    "    if caps_ratio > 0.5:\n",
    "        spam_score += 2\n",
    "    \n",
    "    # Rule 4: Check for suspicious patterns\n",
    "    if '$$$' in email_text or '!!!' in email_text:\n",
    "        spam_score += 2\n",
    "    \n",
    "    return spam_score > 2  # Threshold for spam classification\n",
    "\n",
    "# Test emails\n",
    "test_emails = [\n",
    "    \"Hi there! How are you doing today?\",\n",
    "    \"URGENT!!! You've WON $1000!!! ACT NOW!!!\",\n",
    "    \"Meeting scheduled for tomorrow at 3 PM\",\n",
    "    \"FREE MONEY!!! Limited time offer - click now!!!\"\n",
    "]\n",
    "\n",
    "print(\"üîß Traditional Programming Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    result = is_spam_traditional(email)\n",
    "    print(f\"Email {i}: {'SPAM' if result else 'NOT SPAM'}\")\n",
    "    print(f\"Content: {email[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Approach: Learning from Examples\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training data: emails with labels\n",
    "training_emails = [\n",
    "    (\"Hi John, how was your meeting today?\", 0),  # 0 = not spam\n",
    "    (\"FREE MONEY NOW!!! CLICK HERE!!!\", 1),      # 1 = spam\n",
    "    (\"Reminder: Your appointment is tomorrow\", 0),\n",
    "    (\"You've won $1000000! Act fast!\", 1),\n",
    "    (\"Can we reschedule our lunch?\", 0),\n",
    "    (\"URGENT: Claim your prize now!\", 1),\n",
    "    (\"Thanks for the presentation slides\", 0),\n",
    "    (\"Limited time offer - buy now!\", 1),\n",
    "    (\"Meeting notes attached\", 0),\n",
    "    (\"GET RICH QUICK - GUARANTEED!\", 1)\n",
    "]\n",
    "\n",
    "# Separate texts and labels\n",
    "texts, labels = zip(*training_emails)\n",
    "\n",
    "# Create ML pipeline\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "X_train = vectorizer.fit_transform(texts)\n",
    "classifier.fit(X_train, labels)\n",
    "\n",
    "# Test on the same emails we used before\n",
    "print(\"ü§ñ Machine Learning Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X_test = vectorizer.transform(test_emails)\n",
    "predictions = classifier.predict(X_test)\n",
    "probabilities = classifier.predict_proba(X_test)\n",
    "\n",
    "for i, (email, pred, prob) in enumerate(zip(test_emails, predictions, probabilities), 1):\n",
    "    spam_prob = prob[1]  # probability of being spam\n",
    "    print(f\"Email {i}: {'SPAM' if pred else 'NOT SPAM'} (confidence: {spam_prob:.2f})\")\n",
    "    print(f\"Content: {email[:50]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Key Differences:\")\n",
    "print(\"Traditional: Fixed rules, hard to maintain\")\n",
    "print(\"ML: Learns patterns, adapts to new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c0b56",
   "metadata": {},
   "source": [
    "## Section 3: Supervised Learning in Action\n",
    "\n",
    "Supervised learning uses labeled examples to learn patterns. Let's explore both **classification** (predicting categories) and **regression** (predicting numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d816714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Example 1: Classification (Iris Dataset)\n",
    "print(\"üå∏ Classification Example: Iris Flower Species\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the famous Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "classifier = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Feature scatter plot\n",
    "plt.subplot(1, 3, 1)\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    mask = y == i\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=color, label=iris.target_names[i], alpha=0.7)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Iris Dataset - Sepal Dimensions')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Feature importance\n",
    "plt.subplot(1, 3, 2)\n",
    "importance = classifier.feature_importances_\n",
    "plt.bar(range(len(importance)), importance)\n",
    "plt.xticks(range(len(importance)), [name.split()[0] for name in iris.feature_names], rotation=45)\n",
    "plt.title('Feature Importance')\n",
    "plt.ylabel('Importance')\n",
    "\n",
    "# Plot 3: Confusion matrix-like visualization\n",
    "plt.subplot(1, 3, 3)\n",
    "unique_labels = np.unique(y_test)\n",
    "correct_pred = y_test == y_pred\n",
    "colors_pred = ['green' if correct else 'red' for correct in correct_pred]\n",
    "plt.scatter(range(len(y_test)), y_test, c=colors_pred, alpha=0.7)\n",
    "plt.xlabel('Test Sample')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Predictions (Green=Correct, Red=Wrong)')\n",
    "plt.yticks(unique_labels, iris.target_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Sample Predictions:\")\n",
    "for i in range(min(5, len(X_test))):\n",
    "    print(f\"Sample {i+1}: {iris.target_names[y_test[i]]} ‚Üí Predicted: {iris.target_names[y_pred[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de084eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Example 2: Regression (Boston Housing)\n",
    "print(\"üè† Regression Example: Boston Housing Prices\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create synthetic housing data (since Boston dataset is deprecated)\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate features\n",
    "crime_rate = np.random.exponential(2, n_samples)\n",
    "rooms = np.random.normal(6.5, 1, n_samples)\n",
    "age = np.random.uniform(0, 100, n_samples)\n",
    "distance = np.random.exponential(3, n_samples)\n",
    "\n",
    "# Create target variable with realistic relationships\n",
    "price = (50 - 3 * crime_rate + 8 * rooms - 0.1 * age - 2 * distance + \n",
    "         np.random.normal(0, 5, n_samples))\n",
    "price = np.clip(price, 10, 50)  # Reasonable price range\n",
    "\n",
    "# Combine features\n",
    "X = np.column_stack([crime_rate, rooms, age, distance])\n",
    "feature_names = ['Crime Rate', 'Avg Rooms', 'Building Age', 'Distance to Center']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Price range: ${price.min():.1f}k - ${price.max():.1f}k\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, price, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Linear Regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}k\")\n",
    "print(f\"R¬≤ Score: {regressor.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($k)')\n",
    "plt.ylabel('Predicted Price ($k)')\n",
    "plt.title('Actual vs Predicted Prices')\n",
    "correlation = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes)\n",
    "\n",
    "# Plot 2: Feature coefficients\n",
    "plt.subplot(1, 3, 2)\n",
    "coefficients = regressor.coef_\n",
    "colors = ['red' if coef < 0 else 'green' for coef in coefficients]\n",
    "bars = plt.bar(range(len(coefficients)), coefficients, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(coefficients)), feature_names, rotation=45)\n",
    "plt.title('Feature Coefficients')\n",
    "plt.ylabel('Impact on Price')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals\n",
    "plt.subplot(1, 3, 3)\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($k)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèòÔ∏è Sample Predictions:\")\n",
    "for i in range(min(5, len(X_test))):\n",
    "    print(f\"House {i+1}: Actual ${y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]:.1f}k ‚Üí Predicted ${y_pred[i]:.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4178d",
   "metadata": {},
   "source": [
    "## Section 4: Unsupervised Learning Adventures\n",
    "\n",
    "Unsupervised learning finds hidden patterns in data without labels. Let's explore **clustering** (finding groups) and **dimensionality reduction** (simplifying data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9187f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Learning Example 1: Clustering (Customer Segmentation)\n",
    "print(\"üë• Clustering Example: Customer Segmentation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate synthetic customer data\n",
    "np.random.seed(42)\n",
    "n_customers = 300\n",
    "\n",
    "# Create customer segments with different spending patterns\n",
    "# Segment 1: Budget customers (low income, low spending)\n",
    "budget_customers = np.random.multivariate_normal([25, 20], [[50, 10], [10, 30]], 100)\n",
    "\n",
    "# Segment 2: Premium customers (high income, high spending) \n",
    "premium_customers = np.random.multivariate_normal([70, 80], [[40, 20], [20, 50]], 100)\n",
    "\n",
    "# Segment 3: Middle-tier customers\n",
    "middle_customers = np.random.multivariate_normal([45, 50], [[30, 5], [5, 40]], 100)\n",
    "\n",
    "# Combine all customers\n",
    "X_customers = np.vstack([budget_customers, premium_customers, middle_customers])\n",
    "\n",
    "# Apply K-means clustering (without knowing the true segments)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_customers)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print(f\"Dataset shape: {X_customers.shape}\")\n",
    "print(f\"Number of clusters: {kmeans.n_clusters}\")\n",
    "print(f\"Inertia (within-cluster sum of squares): {kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Original data (without clustering)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_customers[:, 0], X_customers[:, 1], alpha=0.6, c='gray')\n",
    "plt.xlabel('Annual Income ($k)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.title('Customer Data (Before Clustering)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Clustered data\n",
    "plt.subplot(1, 3, 2)\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    plt.scatter(X_customers[mask, 0], X_customers[mask, 1], \n",
    "               c=colors[i], alpha=0.6, label=f'Cluster {i+1}')\n",
    "\n",
    "# Plot centroids\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], \n",
    "           c='black', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "plt.xlabel('Annual Income ($k)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.title('Customer Segmentation (K-Means)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cluster characteristics\n",
    "plt.subplot(1, 3, 3)\n",
    "cluster_stats = []\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    cluster_data = X_customers[mask]\n",
    "    avg_income = cluster_data[:, 0].mean()\n",
    "    avg_spending = cluster_data[:, 1].mean()\n",
    "    cluster_stats.append([avg_income, avg_spending])\n",
    "\n",
    "cluster_stats = np.array(cluster_stats)\n",
    "x_pos = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x_pos - width/2, cluster_stats[:, 0], width, label='Avg Income', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, cluster_stats[:, 1], width, label='Avg Spending', alpha=0.7)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Cluster Characteristics')\n",
    "plt.xticks(x_pos, [f'Cluster {i+1}' for i in range(3)])\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze clusters\n",
    "print(f\"\\nüìä Cluster Analysis:\")\n",
    "segment_names = ['Budget Shoppers', 'Premium Customers', 'Value Seekers']\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    cluster_data = X_customers[mask]\n",
    "    print(f\"\\nCluster {i+1} ({segment_names[i]}):\")\n",
    "    print(f\"  Size: {np.sum(mask)} customers\")\n",
    "    print(f\"  Avg Income: ${cluster_data[:, 0].mean():.1f}k\")\n",
    "    print(f\"  Avg Spending: {cluster_data[:, 1].mean():.1f}\")\n",
    "    print(f\"  Income Range: ${cluster_data[:, 0].min():.1f}k - ${cluster_data[:, 0].max():.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36440ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Learning Example 2: Dimensionality Reduction (PCA)\n",
    "print(\"üìâ Dimensionality Reduction Example: PCA on Iris Dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the Iris dataset (4 dimensions)\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(f\"Original dimensions: {X_iris.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_iris)\n",
    "\n",
    "print(f\"Reduced dimensions: {X_pca.shape}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance retained: {pca.explained_variance_ratio_.sum():.3f} ({pca.explained_variance_ratio_.sum()*100:.1f}%)\")\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# Plot 1: Original data (first 2 features)\n",
    "plt.subplot(1, 4, 1)\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    mask = y_iris == i\n",
    "    plt.scatter(X_iris[mask, 0], X_iris[mask, 1], c=color, label=iris.target_names[i], alpha=0.7)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title('Original Data\\n(First 2 Features)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Original data (last 2 features)\n",
    "plt.subplot(1, 4, 2)\n",
    "for i, color in enumerate(colors):\n",
    "    mask = y_iris == i\n",
    "    plt.scatter(X_iris[mask, 2], X_iris[mask, 3], c=color, label=iris.target_names[i], alpha=0.7)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.title('Original Data\\n(Last 2 Features)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: PCA transformed data\n",
    "plt.subplot(1, 4, 3)\n",
    "for i, color in enumerate(colors):\n",
    "    mask = y_iris == i\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, label=iris.target_names[i], alpha=0.7)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('PCA Transformed Data\\n(2 Principal Components)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Explained variance\n",
    "plt.subplot(1, 4, 4)\n",
    "# Show explained variance for all possible components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_iris)\n",
    "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.bar(range(1, 5), pca_full.explained_variance_ratio_, alpha=0.7, label='Individual')\n",
    "plt.plot(range(1, 5), cumsum_var, 'ro-', label='Cumulative')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show PCA components (loadings)\n",
    "print(f\"\\nüßÆ Principal Components (Feature Loadings):\")\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=iris.feature_names\n",
    ")\n",
    "print(components_df.round(3))\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"‚Ä¢ PC1 explains {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"‚Ä¢ PC2 explains {pca.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "print(f\"‚Ä¢ Together they capture {pca.explained_variance_ratio_.sum():.1%} of information\")\n",
    "print(f\"‚Ä¢ We reduced 4D data to 2D while keeping most information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b989e8",
   "metadata": {},
   "source": [
    "## Section 5: Reinforcement Learning Playground\n",
    "\n",
    "Reinforcement learning is like teaching a computer to play a game by trial and error. Let's create a simple example where an agent learns to make optimal choices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa2dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning Example: Multi-Armed Bandit Problem\n",
    "print(\"üé∞ Reinforcement Learning Example: Multi-Armed Bandit\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    \"\"\"Simulation of a multi-armed bandit (like slot machines)\"\"\"\n",
    "    def __init__(self, n_arms=4, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.n_arms = n_arms\n",
    "        # Each arm has a different reward probability (unknown to agent)\n",
    "        self.true_rewards = np.random.uniform(0.1, 0.9, n_arms)\n",
    "        print(f\"üéØ True reward probabilities (hidden from agent): {self.true_rewards.round(3)}\")\n",
    "        \n",
    "    def pull_arm(self, arm):\n",
    "        \"\"\"Pull an arm and get reward (1) or no reward (0)\"\"\"\n",
    "        return 1 if np.random.random() < self.true_rewards[arm] else 0\n",
    "\n",
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"Agent that learns using epsilon-greedy strategy\"\"\"\n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.counts = np.zeros(n_arms)  # How many times each arm was pulled\n",
    "        self.values = np.zeros(n_arms)  # Estimated value of each arm\n",
    "        self.total_reward = 0\n",
    "        self.history = []\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"Choose which arm to pull (explore vs exploit)\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose random arm\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # Exploit: choose best arm so far\n",
    "            return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Update our knowledge after pulling an arm\"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        # Running average: new_avg = old_avg + (new_value - old_avg) / count\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.total_reward += reward\n",
    "        self.history.append((arm, reward, self.values.copy()))\n",
    "\n",
    "# Set up the experiment\n",
    "bandit = MultiArmedBandit(n_arms=4, seed=42)\n",
    "agent = EpsilonGreedyAgent(n_arms=4, epsilon=0.1)\n",
    "\n",
    "n_rounds = 1000\n",
    "print(f\"\\nü§ñ Agent will play {n_rounds} rounds with epsilon={agent.epsilon}\")\n",
    "print(f\"Epsilon-greedy strategy: {(1-agent.epsilon)*100:.0f}% exploitation, {agent.epsilon*100:.0f}% exploration\")\n",
    "\n",
    "# Run the simulation\n",
    "rewards = []\n",
    "arm_choices = []\n",
    "cumulative_rewards = []\n",
    "\n",
    "for round_num in range(n_rounds):\n",
    "    # Agent chooses an arm\n",
    "    chosen_arm = agent.select_arm()\n",
    "    arm_choices.append(chosen_arm)\n",
    "    \n",
    "    # Pull the arm and get reward\n",
    "    reward = bandit.pull_arm(chosen_arm)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # Agent updates its knowledge\n",
    "    agent.update(chosen_arm, reward)\n",
    "    cumulative_rewards.append(agent.total_reward)\n",
    "\n",
    "print(f\"\\nüìä Results after {n_rounds} rounds:\")\n",
    "print(f\"Total reward: {agent.total_reward}\")\n",
    "print(f\"Average reward per round: {agent.total_reward/n_rounds:.3f}\")\n",
    "print(f\"Agent's learned values: {agent.values.round(3)}\")\n",
    "print(f\"Times each arm was pulled: {agent.counts.astype(int)}\")\n",
    "\n",
    "# Find the optimal strategy (for comparison)\n",
    "optimal_arm = np.argmax(bandit.true_rewards)\n",
    "optimal_reward_rate = bandit.true_rewards[optimal_arm]\n",
    "print(f\"\\nüèÜ Optimal strategy:\")\n",
    "print(f\"Best arm: {optimal_arm} (reward rate: {optimal_reward_rate:.3f})\")\n",
    "print(f\"Agent's performance: {(agent.total_reward/n_rounds)/optimal_reward_rate:.1%} of optimal\")\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# Plot 1: Learning curve\n",
    "plt.subplot(1, 4, 1)\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, n_rounds), moving_avg, label='Agent Performance', linewidth=2)\n",
    "plt.axhline(y=optimal_reward_rate, color='red', linestyle='--', \n",
    "            label=f'Optimal Rate ({optimal_reward_rate:.3f})', alpha=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title(f'Learning Curve\\n(Moving Average, Window={window})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative rewards\n",
    "plt.subplot(1, 4, 2)\n",
    "optimal_cumulative = np.arange(1, n_rounds+1) * optimal_reward_rate\n",
    "plt.plot(cumulative_rewards, label='Agent', linewidth=2)\n",
    "plt.plot(optimal_cumulative, '--', label='Optimal Strategy', alpha=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Arm selection over time\n",
    "plt.subplot(1, 4, 3)\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "for arm in range(bandit.n_arms):\n",
    "    arm_selections = [i for i, choice in enumerate(arm_choices) if choice == arm]\n",
    "    arm_rewards = [i for i in arm_selections]  # Just the round numbers\n",
    "    if arm_selections:\n",
    "        plt.scatter(arm_selections, [arm]*len(arm_selections), \n",
    "                   c=colors[arm], alpha=0.6, s=1, label=f'Arm {arm}')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Arm Chosen')\n",
    "plt.title('Arm Selection Over Time')\n",
    "plt.legend()\n",
    "plt.yticks(range(bandit.n_arms))\n",
    "\n",
    "# Plot 4: Final comparison\n",
    "plt.subplot(1, 4, 4)\n",
    "x_pos = np.arange(bandit.n_arms)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = plt.bar(x_pos - width/2, bandit.true_rewards, width, \n",
    "               label='True Rewards', alpha=0.7, color='blue')\n",
    "bars2 = plt.bar(x_pos + width/2, agent.values, width, \n",
    "               label='Learned Values', alpha=0.7, color='orange')\n",
    "\n",
    "# Add pull counts as text\n",
    "for i, count in enumerate(agent.counts):\n",
    "    plt.text(i, max(bandit.true_rewards[i], agent.values[i]) + 0.05, \n",
    "             f'{int(count)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Arm')\n",
    "plt.ylabel('Reward Probability')\n",
    "plt.title('True vs Learned Values\\n(Numbers show pull counts)')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show exploration vs exploitation decisions\n",
    "exploration_count = sum(1 for i, choice in enumerate(arm_choices) \n",
    "                       if choice != np.argmax(agent.history[i][2]) and i > 0)\n",
    "print(f\"\\nüîç Decision Analysis:\")\n",
    "print(f\"Exploration decisions: {exploration_count} ({exploration_count/n_rounds:.1%})\")\n",
    "print(f\"Exploitation decisions: {n_rounds - exploration_count} ({(n_rounds-exploration_count)/n_rounds:.1%})\")\n",
    "print(f\"Most pulled arm: {np.argmax(agent.counts)} (optimal: {optimal_arm})\")\n",
    "\n",
    "if np.argmax(agent.counts) == optimal_arm:\n",
    "    print(\"üéâ Success! Agent learned to prefer the optimal arm!\")\n",
    "else:\n",
    "    print(\"ü§î Agent didn't fully converge to optimal arm. Try more rounds or different epsilon!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a9947",
   "metadata": {},
   "source": [
    "## Section 6: Python Libraries Mastery\n",
    "\n",
    "Let's dive deeper into the essential Python libraries for machine learning. Understanding these tools is crucial for your ML journey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e605fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries Deep Dive\n",
    "print(\"üêç Essential Python Libraries for Machine Learning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. NumPy: Numerical Computing Foundation\n",
    "print(\"\\nüìä 1. NumPy - Numerical Computing\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create arrays and perform operations\n",
    "arr_1d = np.array([1, 2, 3, 4, 5])\n",
    "arr_2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "arr_random = np.random.randn(1000)  # 1000 random numbers\n",
    "\n",
    "print(f\"1D array: {arr_1d}\")\n",
    "print(f\"2D array shape: {arr_2d.shape}\")\n",
    "print(f\"Array operations: mean={np.mean(arr_random):.3f}, std={np.std(arr_random):.3f}\")\n",
    "\n",
    "# Mathematical operations\n",
    "print(f\"Broadcasting: [1,2,3] * 2 = {arr_1d[:3] * 2}\")\n",
    "print(f\"Matrix multiplication: \\n{arr_2d @ arr_2d.T}\")\n",
    "\n",
    "# 2. Pandas: Data Manipulation Powerhouse\n",
    "print(\"\\nüêº 2. Pandas - Data Manipulation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': np.random.randint(22, 65, 5),\n",
    "    'Salary': np.random.randint(40000, 120000, 5),\n",
    "    'Department': np.random.choice(['Engineering', 'Sales', 'Marketing'], 5),\n",
    "    'Performance': np.random.uniform(3.0, 5.0, 5).round(1)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample Dataset:\")\n",
    "print(df)\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "print(f\"\\nSummary Statistics:\\n{df.describe()}\")\n",
    "\n",
    "# Data operations\n",
    "high_performers = df[df['Performance'] > 4.0]\n",
    "dept_salary = df.groupby('Department')['Salary'].mean()\n",
    "print(f\"\\nHigh performers (>4.0): {len(high_performers)}\")\n",
    "print(f\"Average salary by department:\\n{dept_salary}\")\n",
    "\n",
    "# 3. Matplotlib: Visualization Magic\n",
    "print(\"\\nüìà 3. Matplotlib - Data Visualization\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Plot 1: Line plot\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x) + np.random.normal(0, 0.1, 100)\n",
    "axes[0, 0].plot(x, y, alpha=0.7)\n",
    "axes[0, 0].set_title('Noisy Sine Wave')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Histogram\n",
    "axes[0, 1].hist(arr_random, bins=30, alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('Random Distribution')\n",
    "axes[0, 1].axvline(np.mean(arr_random), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Scatter plot\n",
    "axes[0, 2].scatter(df['Age'], df['Salary'], c=df['Performance'], \n",
    "                   cmap='viridis', s=100, alpha=0.7)\n",
    "axes[0, 2].set_xlabel('Age')\n",
    "axes[0, 2].set_ylabel('Salary')\n",
    "axes[0, 2].set_title('Age vs Salary (colored by Performance)')\n",
    "\n",
    "# Plot 4: Bar chart\n",
    "dept_counts = df['Department'].value_counts()\n",
    "axes[1, 0].bar(dept_counts.index, dept_counts.values, alpha=0.7)\n",
    "axes[1, 0].set_title('Department Distribution')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 5: Box plot\n",
    "departments = df['Department'].unique()\n",
    "salary_by_dept = [df[df['Department'] == dept]['Salary'].values for dept in departments]\n",
    "axes[1, 1].boxplot(salary_by_dept, labels=departments)\n",
    "axes[1, 1].set_title('Salary Distribution by Department')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 6: Heatmap-style correlation\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "correlation = numeric_df.corr()\n",
    "im = axes[1, 2].imshow(correlation, cmap='coolwarm', aspect='auto')\n",
    "axes[1, 2].set_xticks(range(len(correlation.columns)))\n",
    "axes[1, 2].set_yticks(range(len(correlation.columns)))\n",
    "axes[1, 2].set_xticklabels(correlation.columns, rotation=45)\n",
    "axes[1, 2].set_yticklabels(correlation.columns)\n",
    "axes[1, 2].set_title('Correlation Matrix')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(correlation.columns)):\n",
    "    for j in range(len(correlation.columns)):\n",
    "        axes[1, 2].text(j, i, f'{correlation.iloc[i, j]:.2f}', \n",
    "                        ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Scikit-learn: Machine Learning Made Easy\n",
    "print(\"\\nüéØ 4. Scikit-learn - Machine Learning\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a complete ML pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create a pipeline (preprocessing + model)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores.round(3)}\")\n",
    "print(f\"Average CV score: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Train final model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pipeline.named_steps['classifier'].feature_importances_\n",
    "print(f\"\\nFeature importance:\")\n",
    "for name, importance in zip(iris.feature_names, feature_importance):\n",
    "    print(f\"  {name}: {importance:.3f}\")\n",
    "\n",
    "# Make predictions on new samples\n",
    "new_samples = np.array([[5.0, 3.0, 1.5, 0.2], [6.5, 3.0, 5.5, 2.0]])\n",
    "predictions = pipeline.predict(new_samples)\n",
    "probabilities = pipeline.predict_proba(new_samples)\n",
    "\n",
    "print(f\"\\nPredictions on new samples:\")\n",
    "for i, (sample, pred, probs) in enumerate(zip(new_samples, predictions, probabilities)):\n",
    "    print(f\"Sample {i+1}: {sample}\")\n",
    "    print(f\"  Predicted: {iris.target_names[pred]}\")\n",
    "    print(f\"  Probabilities: {dict(zip(iris.target_names, probs.round(3)))}\")\n",
    "\n",
    "print(f\"\\nüéì Library Summary:\")\n",
    "print(\"‚Ä¢ NumPy: Fast numerical operations, array handling\")\n",
    "print(\"‚Ä¢ Pandas: Data manipulation, analysis, file I/O\")\n",
    "print(\"‚Ä¢ Matplotlib: Plotting, visualization, charts\")\n",
    "print(\"‚Ä¢ Scikit-learn: Machine learning algorithms, evaluation tools\")\n",
    "print(\"\\nüöÄ These four libraries form the foundation of most ML projects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78f75d",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've completed the Introduction to Machine Learning chapter. \n",
    "\n",
    "### What You've Learned:\n",
    "- ‚úÖ **Traditional vs ML Programming**: Seen the fundamental difference in approaches\n",
    "- ‚úÖ **Supervised Learning**: Explored classification and regression with real examples\n",
    "- ‚úÖ **Unsupervised Learning**: Discovered clustering and dimensionality reduction\n",
    "- ‚úÖ **Reinforcement Learning**: Watched an agent learn through trial and error\n",
    "- ‚úÖ **Python ML Stack**: Mastered NumPy, Pandas, Matplotlib, and Scikit-learn\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **ML learns patterns** from data rather than following explicit rules\n",
    "2. **Different types of ML** solve different types of problems\n",
    "3. **Python provides powerful tools** for every aspect of ML\n",
    "4. **Visualization is crucial** for understanding data and results\n",
    "5. **Practice makes perfect** - the more you experiment, the better you become\n",
    "\n",
    "### What's Next?\n",
    "In **Chapter 2: Data Preprocessing**, you'll learn:\n",
    "- How to clean messy, real-world data\n",
    "- Techniques for handling missing values\n",
    "- Methods for splitting datasets effectively\n",
    "- Best practices for data preparation\n",
    "\n",
    "### Practice Exercises:\n",
    "Try these to reinforce your learning:\n",
    "1. **Modify the spam detector** to include new rules or features\n",
    "2. **Experiment with different datasets** using the same ML techniques\n",
    "3. **Change parameters** in the examples and observe the effects\n",
    "4. **Create your own visualizations** using different plot types\n",
    "\n",
    "### Resources for Continued Learning:\n",
    "- üìö Scikit-learn documentation: https://scikit-learn.org/\n",
    "- üé• Machine Learning courses on Coursera and edX\n",
    "- üìä Practice datasets: Kaggle.com\n",
    "- üí¨ Community: Stack Overflow, Reddit r/MachineLearning\n",
    "\n",
    "Happy learning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
