# INDEX

---

*Comprehensive alphabetical reference for all topics, concepts, algorithms, and terms covered in this textbook.*

---

## A

**Accuracy**, 45, 78, 156, 189, 245  
- Cross-validation accuracy, 201-205  
- Training vs. testing accuracy, 167-169  

**Activation Functions**, 89, 156, 234  
- ReLU, 234  
- Sigmoid, 156-159, 234  
- Tanh, 234  

**Algorithms**  
- Classification algorithms, 45-78  
- Comparison of algorithms, 189-195  
- Selection guidelines, 195-197  

**Artificial Intelligence (AI)**, 1-15, 301-315  
- Definition and scope, 1-3  
- History of AI, 305-308  
- Narrow vs. General AI, 302-303  
- Types of AI, 2-3  

**Artificial Neural Networks**, 234-240  
- Backpropagation, 237-238  
- Forward propagation, 236-237  
- Multi-layer perceptrons, 235-236  

**AUC (Area Under Curve)**, 178-182  

---

## B

**Bagging**, 67-69  
- Bootstrap aggregating, 67  
- Random Forest, 67-70  

**Bias**, 34, 167, 234  
- Bias-variance tradeoff, 167-168  
- In machine learning models, 34-35  

**Binary Classification**, 45-47, 156-159  

**Bootstrap Sampling**, 67-68  

---

## C

**Classification**, 45-197  
- Binary classification, 45-47  
- Multi-class classification, 47-48, 162-164  
- Multi-label classification, 48  
- Evaluation metrics, 172-189  

**Classification Algorithms**  
- Decision Trees, 49-70  
- K-Nearest Neighbors, 71-98  
- Logistic Regression, 156-170  
- Random Forest, 67-70  
- Support Vector Machines, 99-155  

**Clustering**, 245-267  
- Hierarchical clustering, 256-259  
- K-Means clustering, 245-255  
- DBSCAN, 259-262  

**Confusion Matrix**, 172-175  

**Cost Function**, 156-159, 234  

**Cross-Validation**, 26-30, 183-186  
- K-fold cross-validation, 27-29  
- Leave-one-out cross-validation, 29  
- Stratified cross-validation, 29-30  

**Curse of Dimensionality**, 98, 117-119  

---

## D

**Data Preprocessing**, 16-44  
- Data cleaning, 17-22  
- Feature scaling, 31-35  
- Handling missing values, 22-26  
- Train-test split, 26-30  

**Data Quality**, 17-22  
- Outlier detection, 19-21  
- Data validation, 21-22  

**Datasets**  
- Boston Housing, 198, 220-225  
- Breast Cancer, 52-56, 99-105  
- Iris, 1, 52-56, 71-75, 172-189  
- Wine, 75-78, 195-197  

**Decision Trees**, 49-70  
- Entropy, 53-55  
- Gini impurity, 55-56  
- Information gain, 53-55  
- Overfitting and pruning, 56-61  
- Visualization, 52-53  

**Deep Learning**, 234-240, 308-310  
- Convolutional Neural Networks, 238-240  
- Neural networks, 234-238  

**Dimensionality Reduction**, 117-134, 267-289  
- Linear Discriminant Analysis (LDA), 128-134  
- Principal Component Analysis (PCA), 117-128  
- t-SNE, 278-282  

---

## E

**Ensemble Methods**, 67-70  
- Bagging, 67-69  
- Boosting, 69-70  
- Random Forest, 67-70  

**Entropy**, 53-55  

**Evaluation Metrics**  
- Classification metrics, 172-189  
- Regression metrics, 225-232  

**Exploratory Data Analysis (EDA)**, 17-19  

---

## F

**F1-Score**, 176-178  

**Feature Engineering**, 99-155  
- Feature creation, 134-140  
- Feature extraction, 117-134  
- Feature scaling, 103-109  
- Feature selection, 109-117  

**Feature Selection**, 109-117  
- Embedded methods, 115-117  
- Filter methods, 109-112  
- Wrapper methods, 112-115  

---

## G

**Gradient Descent**, 159-162, 234  

**Grid Search**, 78-81, 152-155  

---

## H

**Hyperparameter Tuning**, 78-81, 152-155  

**Hyperparameters**  
- Decision Trees, 61-64  
- KNN, 81-85  
- SVM, 152-155  

---

## I

**Imputation**, 22-26  
- Mean/median imputation, 23-24  
- Forward/backward fill, 24-25  
- Advanced imputation, 25-26  

**Information Gain**, 53-55  

---

## K

**K-Fold Cross-Validation**, 27-29  

**K-Means Clustering**, 245-255  

**K-Nearest Neighbors (KNN)**, 71-98  
- Distance metrics, 75-78  
- Implementation from scratch, 85-90  
- Optimal K selection, 81-85  
- Weighted KNN, 90-92  

**Kernel Trick**, 140-148  
- Linear kernel, 141  
- Polynomial kernel, 142-144  
- RBF kernel, 144-146  
- Sigmoid kernel, 146-148  

---

## L

**Learning Curves**, 186-189  

**Linear Discriminant Analysis (LDA)**, 128-134  

**Linear Regression**, 198-215  
- Multiple linear regression, 205-210  
- Simple linear regression, 198-205  

**Logistic Regression**, 156-170  
- Cost function, 156-159  
- Multi-class logistic regression, 162-164  
- Regularization, 164-167  
- Sigmoid function, 156-159  

---

## M

**Machine Learning**  
- Definition, 1-3  
- Supervised vs. unsupervised, 3-6  
- Types of learning, 3-6  

**Mean Absolute Error (MAE)**, 225-228  

**Mean Squared Error (MSE)**, 225-228  

**Missing Values**, 22-26  

**Model Evaluation**, 172-189, 225-232  

**Multi-class Classification**, 47-48, 162-164  

**Mutual Information**, 134-137  

---

## N

**Neural Networks**, 234-240  

**Normalization**, 31-35  
- Min-Max normalization, 32-33  
- Z-score normalization, 33-34  

---

## O

**One-Hot Encoding**, 35-38  

**Outliers**, 19-21, 34-35  

**Overfitting**, 56-61, 167-169  
- In decision trees, 56-61  
- Prevention techniques, 61-64  

---

## P

**Precision**, 176-178  

**Principal Component Analysis (PCA)**, 117-128  
- Eigenvalues and eigenvectors, 119-122  
- Implementation, 122-125  
- Interpretation, 125-128  

**Pruning**, 61-64  

**Python Libraries**  
- Matplotlib, 8-12  
- NumPy, 6-8  
- Pandas, 8-10  
- Scikit-learn, 12-15  

---

## R

**R-squared**, 228-232  

**Random Forest**, 67-70  

**Recall**, 176-178  

**Regression**, 198-243  
- Linear regression, 198-215  
- Polynomial regression, 215-220  
- Ridge regression, 220-225  

**Regularization**, 164-167, 220-225  
- L1 regularization (Lasso), 222-225  
- L2 regularization (Ridge), 220-222  

**ROC Curve**, 178-182  

---

## S

**Scaling**, 31-35, 103-109  
- Min-Max scaling, 104-106  
- Robust scaling, 108-109  
- Standard scaling, 106-108  

**SHAP Values**, 137-140  

**Sigmoid Function**, 156-159  

**Supervised Learning**, 3-5, 45-243  

**Support Vector Machines (SVM)**, 99-155  
- Kernel trick, 140-148  
- Linear SVM, 99-105  
- Non-linear SVM, 140-148  
- Parameter tuning, 152-155  

---

## T

**Train-Test Split**, 26-30  

---

## U

**Underfitting**, 167-169  

**Unsupervised Learning**, 5-6, 245-289  

---

## V

**Validation Curves**, 64-67, 152-155  

**Variance**, 167-169  

---

## W

**Weighted KNN**, 90-92  

---

## Symbols and Numbers

**80-20 Rule**, 26-27  

---

## Appendices

**Appendix A**: Mathematical Prerequisites, 290-295  
**Appendix B**: Python Setup Guide, 296-298  
**Appendix C**: Dataset Sources, 299-300  

---

## Algorithms Reference

| Algorithm | Page | Chapter |
|-----------|------|---------|
| Decision Trees | 49-70 | 4 |
| K-Nearest Neighbors | 71-98 | 4 |
| Support Vector Machines | 99-155 | 4 |
| Logistic Regression | 156-170 | 4 |
| Linear Regression | 198-215 | 5 |
| Ridge Regression | 220-225 | 5 |
| Lasso Regression | 222-225 | 5 |
| Random Forest | 67-70 | 4 |
| K-Means Clustering | 245-255 | 6 |
| PCA | 117-128 | 3 |
| LDA | 128-134 | 3 |

---

## Mathematical Concepts

| Concept | Page | Description |
|---------|------|-------------|
| Entropy | 53-55 | Measure of information content |
| Information Gain | 53-55 | Reduction in entropy |
| Euclidean Distance | 75-76 | Standard distance metric |
| Manhattan Distance | 76-77 | L1 distance metric |
| Cosine Similarity | 77-78 | Angle-based similarity |
| Eigenvalues | 119-122 | PCA mathematical foundation |
| Gradient Descent | 159-162 | Optimization algorithm |
| Cross-Entropy | 156-159 | Logistic regression loss |

---

*Page numbers are approximate and may vary in the final printed version.*
